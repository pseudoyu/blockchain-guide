[{"id":0,"href":"/zh/docs/ethereum/blockchain_ethereum_mpt/","title":"Ethereum MPT(Merkle Patricia Tries) 详解","section":"Docs","content":" Ethereum MPT(Merkle Patricia Tries) 详解 # 前言 # 最近接到了一个工作任务，将项目智能合约状态树中的数据结构从红黑树改为字典树，并对比一下两个数据结构的性能，Trie 主要参照的是 Ethereum 官方的 Java 实现 ethereum/ethereumj，而红黑树则是自己实现，本文则是对两个数据结构的理论和实际表现对比的记录。\n数据结构 # Red-Black Tree - 红黑树 # 红黑树是一种近似平衡的二叉查找树，含有红黑结点，能够确保任何一个结点的左右子树高度差小于两倍。\n性质 # 必须满足以下五个性质：\n结点为红色或黑色 根结点为黑色 叶子结点（NIL）为黑色 每个红色节点的两个子结点为黑色 任意一个结点到每个叶子结点的路径都包含相同数量的黑色结点 红黑树并不是完美平衡的，但是左子树和右子树的层数是相等的，因此，也成为黑色完美平衡。因为是近似平衡的，所以旋转的频次会降低，维护成本下降，时间复杂度维持在 LogN。\n操作 # 红黑树主要通过三种操作来保持自平衡：\n左旋 右旋 变色 与 AVL 的对比 # AVL 提供了更快的查找操作（因为完美平衡） 红黑树提供了更快的插入和删除操作 AVL 存储的结点信息更多（平衡因子与高度），因此占存储空间更大 读操作多、写操作少的时候用 AVL 更合适，多用于数据库；当写操作较多时一般使用红黑树，简洁好实现，多用于各类高级语言的库中，如 map、set 等 代码实现 # 因为红黑树较为复杂，实现代码上传至 GitHub 供学习查看。\npseudoyu/Red_Black_Tree_Java - GitHub\nTrie - 字典树 # Trie 被称为字典树，又称单词查找树或键树，常用于统计和排序大量的字符串，如搜索引擎的文本磁盘统计等。\n它能够最大限度减少无谓的字符串比较，查询效率较高。\n性质 # 结点不存完整单词 从根结点到某一结点，路径上经过的字符连接起来为该结点对应的字符串 每个结点的所有子结点路径代表的字符都不相同 结点可以存储额外信息，如词频等 结点内部实现 # 字典树的高度较低，但占用的存储空间较大，核心思想是空间换时间。\n利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的，可以很天然地解决单词联想等业务场景。\n代码实现 # class Trie { private Trie[] children; private boolean isEnd; public Trie() { children = new Trie[26]; isEnd = false; } public void insert(String word) { Trie node = this; for (int i = 0; i \u0026lt; word.length(); i++) { char ch = word.charAt(i); int index = ch - \u0026#39;a\u0026#39;; if (node.children[index] == null) { node.children[index] = new Trie(); } node = node.children[index]; } node.isEnd = true; } public boolean search(String word) { Trie node = searchPrefix(word); return node != null \u0026amp;\u0026amp; node.isEnd; } public boolean startsWith(String prefix) { return searchPrefix(prefix) != null; } private Trie searchPrefix(String prefix) { Trie node = this; for (int i = 0; i \u0026lt; prefix.length(); i++) { char ch = prefix.charAt(i); int index = ch - \u0026#39;a\u0026#39;; if (node.children[index] == null) { return null; } node = node.children[index]; } return node; } } Modified Merkle Patricia Tries # 以太坊账户状态存储方式 # 使用 Key-Value 的哈希表存储在每次出块时都会有新交易打包进块中，从而改变 merkle tree，但事实上只有一小部分账户发生改变，成本过高 直接用 merkle tree 存放账户，要改内容时直接改 merkle tree 也不可行，因为 merkle tree 没有提供一个高校的查找和更新方法 使用 sorted merkle tree 也不可行，因为新增账户产生的账户地址是随机的，需要插入重新排序 MPT 结构 # 利用了 Trie 结构的特点\n打乱顺序后 Trie 结构不变，天然排序，即使插入新值也不影响，适用于以太坊 account-base 的结构 具有很好的更新局部性，更新时不用遍历整棵树 但是 Trie 结构比较浪费存储空间，当键值对分布稀疏时效率较低，而以太坊的账户地址是 40 位十六进制数，地址约为 2^160 种，极其稀疏（防止哈希碰撞）。\n因此，需要对 Trie 结构进行路径压缩，也就是 Pactricia Trie，经过压缩后，树的高度明显减少，空间和效率都得到提升。\nModified MPT 结构 # 而以太坊真正采用的是 Modified MPT 结构，其结构如下\n每次发布新的区块时，状态树中的新节点的值会发生变化，并不是更改原值，而是新建一些分支，保留原来的状态（因此可以实现回滚）。\n在以太坊系统中，分叉是常态，orphan block 中的数据都要向前回滚，而由于 ETH 中有智能合约，为了支持智能合约的回滚，必须保持之前的状态。\n代码实现 # 代码参照以太坊的 Java 实现。\nethereum/ethereumj - GitHub\n总结 # 以上就是对Ethereum MPT 与红黑树数据结构的解析，在刷 LeetCode 痛苦的时候想过很多次这些学了也用不到，没想到那么快就有了应用场景，还是要好好理解和实践呀！\n参考资料 # 30 张图带你彻底理解红黑树 LeetCode 实现 Trie pseudoyu/Red_Black_Tree_Java 以太坊源码分析 \u0026ndash; MPT 树 ethereum/ethereumj "},{"id":1,"href":"/zh/docs/ethereum/blockchain_ethereum_basic/","title":"Ethereum 核心技术解读","section":"Docs","content":" Ethereum 核心技术解读 # 前言 # 比特币作为一种去中心化的数字货币，是极其成功的，但受限于比特币脚本（非图灵完备，只能处理一些简单的逻辑），并不能处理很复杂的业务。而Ethereum引入了智能合约，使去中心化的概念能够应用于更丰富的应用场景，因此也被称为区块链 2.0。本文将对以太坊核心技术进行解读，如有错漏，欢迎交流指正。\nEthereum 系统 # 2014 年 1 月，俄罗斯开发者 Vitalik Buterin 发布了以太坊白皮书并成立团队，旨在创造一个集成更通用的脚本语言的区块链平台。其中一位成员 Dr. Gavin Wood 发布了一份黄皮书，涉及Ethereum Virtual Machin(EVM)以太坊虚拟的相关技术，这就是Ethereum的诞生。\n简单来说，Ethereum是一个开源的去中心化系统，使用区块链来存储系统状态变化，因此也被称为“世界计算机”；它支持开发者在区块链上部署运行不可变的程序，称为智能合约，因此可以支持广泛的应用场景；它使用数字货币Ether来衡量系统资源消耗，激励更多人参与Ethereum系统建设。\n去中心化应用 DApp # 狭义来说，DApp 其实就是一个集成了用户界面、支持智能合约、运行于以太坊区块链上的应用。\n如上图所示，Ethereum应用实例部署在区块链网络上（智能合约运行于区块链虚拟机中），而 Web 程序只需要通过Web3.js对区块链网络进行RPC远程调用，这样用户就可以通过浏览器（DApp 浏览器或 MetaMask 等插件工具）访问去中心化服务应用了。\n账本 # Ethereum区块链是一个去中心化的账本（数据库），网络中的所有交易都会存储在区块链中，所有节点都要本地保存一份数据，并且确保每一笔交易的可信度；所有的交易都是公开且不可篡改的，网络中的所有节点都可以查看和验证。\n账户 # 当我们需要登录一个网站或系统（比如邮箱）时，往往需要一个帐号和一个密码，密码通过加密算法以暗文的形式存储在中心化的数据库中。然而，以太坊是一个去中心化的系统，那是怎么生成账户的呢？\n和比特币系统原理类似\n首先生成一个仅有自己知道的私钥，假设为sk，采用ECDSA(Elliptic Curve Digital Signature Algorithm)椭圆曲线算法生成对应的公钥pk 采用keccak256算法对公钥pk求哈希值 截取后 160 位作为以太坊的地址 用户的私钥和地址一起组成了以太坊的账户，可以存储余额、发起交易等（比特币的余额是通过计算所有的UTXO得到的，而不是像以太坊一样存储在账户中）。\n其实Ethereum账户分为两种类型，上述方式生成的叫Externally Owned Accounts(EOA)，外部账户，也就是常规用户拥有的账户，主要是用来发送/接收Ether代币或者向智能合约发送交易（即调用智能合约）。\n而另一种则是Contract Accounts，合约账户，不同于外部账户，这种账户是没有对应的私钥的，而是在部署合约的时候生成的，存储智能合约代码。值得注意的是，合约账户必须要被外部账户或者其他合约调用才能够发送或接收Ether，而不能自己主动执行交易。\n钱包 # 存储和管理Ethereum账户的软件/插件称为钱包，提供了诸如交易签名、余额管理等功能。钱包生成主要有两种方式，非确定性随机生成或根据随机种子生成。\nGas # Ethereum网络上的操作也需要“手续费”，称为Gas，在区块链上部署智能合约以及转账都需要消耗一定单位的Gas，这也是鼓励矿工参与Ethereum网络建设的激励机制，从而使整个网络更加安全、可靠。\n每个交易都可以设置相应的Gas量和Gas的价格，设置较高的Gas费则往往矿工会更快处理你的交易，但为了预防交易多次执行消耗大量Gas费，可以通过Gas Limit来设置限制。Gas相关信息可以通过 Ethereum Gas Tracker 工具进行查询。\nIf START_GAS * GAS_PRICE \u0026gt; caller.balance, halt Deduct START_GAS * GAS_PRICE from caller.balance Set GAS = START_GAS Run code, deducting from GAS For negative values, add to GAS_REFUND After termination, add GAS_REFUND to caller.balance 智能合约 # 上文提到，Ethereum区块链不仅仅存储交易信息，还会存储与执行智能合约代码。\n智能合约控制应用和交易逻辑，Ethereum系统中的智能合约采用专属Solidity语言，语法类似于JavaScript，除此之外，还有Vyper、Bamboo等编程语言。智能合约代码会被编译为字节码并部署至区块链中，一旦上链则不可以再编辑。EVM作为一个智能合约执行环境，能够保障执行结果的确定性。\n智能合约示例：众筹 # 让我们想象一个更复杂的场景，假设我要众筹 10000 元开发一个新产品，通过现有众筹平台需要支付不菲的手续费，而且很难解决信任问题，于是，可以通过一个众筹的 DApp 来解决这个问题。\n先为众筹设置一些规则\n每个想参与众筹的人可以捐款 10-10000 元的金额 如果目标金额达成了，金额会通过智能合约发送给我（即众筹发起人） 如果目标在一定时间内（如 1 个月）没有达成，众筹的资金会原路返回至众筹用户 也可以设置一些规则，比如一周后，如果目标金额没有达成，用户可以申请退款 因为这些众筹条款是通过智能合约实现并部署在公开的区块链上的，即使是发起者也不能篡改条款，且任何人都可以查看，解决了信任问题。\n完整代码可以点击这里查看：Demo\n交易 # 在Ethereum中，一个典型的交易是怎么样的呢？\n开发者部署智能合约至区块链 DApp 实例化合约、传入相应值以执行合约 DApp 对交易进行数字签名 本地对交易进行验证 广播交易至网络中 矿工节点接收交易并进行验证 矿工节点确认可信区块后广播至网络中 本地节点与网络进行同步，接收新区块 架构 # Ethereum采用的是一种Order - Execute - Validate - Update State的系统架构。在这种架构下，当产生一笔新的交易，矿工会进行PoW工作量证明机制的运算；验证完成后，将区块通过gossip协议广播至网络中；网络中的其他节点接收到新区块后，也会对区块进行验证；最终，提交至区块链，更新状态。\n具体来看，Ethereum系统有共识层、数据层、应用层等核心组件，其交互逻辑如下：\n如上图所示，Ethereum数据由Transaction Root和State Root组成。Transaction Root是所有交易组成的树，包含From、To、Data、Value、Gas Limit和Gas Price；而State Root则是所有账户组成的树，包含Address、Code、Storage、Balance和Nonce。\n总结 # 以上就是对Ethereum核心技术的一些解读，智能合约的引入给区块链的应用带来了更多可能性，但仍有很多安全性、隐私性和效率问题需要考虑。针对复杂的企业级应用场景，联盟链是更好的选择，后续将会对Hyperledger Fabric进行详尽的分析，敬请期待！\n参考资料 # COMP7408 Distributed Ledger and Blockchain Technology, Professor S.M. Yiu, HKU Udacity Blockchain Developer Nanodegree, Udacity 区块链技术与应用，肖臻，北京大学 区块链技术进阶与实战，蔡亮 李启雷 梁秀波，浙江大学 | 趣链科技 Ethereum Architecture, zastrin Learn Solidity: Complete Example: Crowd Funding Smart Contract, TOSHBLOCKS "},{"id":2,"href":"/zh/docs/hyperledger_fabric/blockchain_hyperledger_fabric_gosdk_event/","title":"Hyperledger Fabric Go SDK 事件分析","section":"Docs","content":" Hyperledger Fabric Go SDK 事件分析 # 前言 # 最近在做跨链适配器，需要在一条本地链上利用 Go SDK 来连接 fabric 网络，并监听事件，所以对 fabric 所支持的事件与 SDK 所提供的监听方法做一下汇总。\nFabric 事件 # 事件类型 # 事件是客户端与 Fabric 网络进行交互的一种方式，如上图所示，事件主要由 Ledger 和存有链码合约的容器触发。Fabric 共支持四种事件形式：\nBlockEvent 监控新增到 fabric 上的块时使用 ChaincodeEvent 监控链码中发布的事件时使用，也就是用户自定义事件 TxStatusEvent 监控节点上的交易完成时使用 FilteredBlockEvent 监控简要的区块信息 在 Fabric Go SDK 中则通过以下几种事件监听器进行操作\nfunc (c *Client) RegisterBlockEvent(filter ...fab.BlockFilter) (fab.Registration, \u0026lt;-chan *fab.BlockEvent, error) func (c *Client) RegisterChaincodeEvent(ccID, eventFilter string) (fab.Registration, \u0026lt;-chan *fab.CCEvent, error) func (c *Client) RegisterFilteredBlockEvent() (fab.Registration, \u0026lt;-chan *fab.FilteredBlockEvent, error) func (c *Client) RegisterTxStatusEvent(txID string) (fab.Registration, \u0026lt;-chan *fab.TxStatusEvent, error) 而当监听完成后需要通过 func (c *Client) Unregister(reg fab.Registration) 来取消注册并移除事件通道\n事件实现过程 # 实现时间过程需要两个步骤\n在链码中调用 SetEvent 方法 在在客户端中通过 Go SDK 实现事件监听器 SetEvent 方法 # 方法定义\nfunc (s *ChaincodeStub) SetEvent(name string, payload []byte) error 调用实例\nfunc (s *SmartContract) Invoke(stub shim.ChaincodeStubInterface) sc.Response { err = stub.PutState(key, value) if err != nil { return shim.Error(fmt.Sprintf(\u0026#34;unable put state (%s), error: %v\u0026#34;, key, err)) } // Payload 需要转换为字节格式 eventPayload := \u0026#34;Event Information\u0026#34; payloadAsBytes := []byte(eventPayload) // SetEvent 方法通常位于 PutState、DelState 等与账本交互的操作之后 err = stub.SetEvent(\u0026#34;\u0026lt;事件名称\u0026gt;\u0026#34;, payloadAsBytes) if (eventErr != nil) { return shim.Error(fmt.Sprintf(\u0026#34;事件触发失败\u0026#34;)) } return shim.Success(nil) } 客户端事件监听器 # // 实现一个链码事件监听 // 传入相应参数，这里的 eventId 必须与链码里的 \u0026lt;事件名称\u0026gt; 匹配以实现监听 reg, eventChannel, err := eventClient.RegisterChaincodeEvent(chaincodeID, eventID) if err != nil { log.Fatalf(\u0026#34;Failed to regitser block event: %v\\n\u0026#34;, err) return } // 取消注册并移除事件通道 defer eventClient.Unregister(reg) 总结 # 以上就是通过 Go SDK 对 fabric 网络上的事件进行监听操作的基本介绍，正在看 fabric Go SDK 源码，后续将补充一些解读。\n参考资料 # hyperledger/fabric-sdk-go Hyperledger Fabric Packages for Go Chaincode fabric 支持的事件 Fabric 1.4 源码解读 3：事件(Event)原理解读 如何监听 Fabric 链码的事件 "},{"id":3,"href":"/zh/docs/hyperledger_fabric/blockchain_hyperledger_fabric_network/","title":"Hyperledger Fabric 网络与安全体系浅析","section":"Docs","content":" Hyperledger Fabric 网络与安全体系浅析 # 前言 # 上一篇文章《Hyperledger Fabric 架构详解》对Fabric的架构和工作原理进行了详细的解读与分析，那作为一个企业级的区块链系统，它是如何根据复杂的业务需求搭建网络，在运行过程中存在哪些安全问题，以及Fabric是如何从机制上进行预防的呢？\n本文将通过实例阐释一个简化版的企业Fabric网络是如何构建的，并对其网络与安全体系进行分析，如有错漏，欢迎交流指正。\nHyperledger Fabric 网络 # Hyperledger Fabric 应用场景实例 # 业务角色 # 假设有一个采用Fabric系统的应用场景里。\n有 4 个组织R1, R2, R3和R4，R4是网络启动者，R1和R4共同担任网络管理员角色。\n系统设置了 2 个通道，分别为C1和C2。R1和R2使用C1通道，R2和R3使用C2通道。\n应用A1属于组织R1，于C1通道运行；应用A2属于组织R2，同时于C1通道和C2通道运行；应用A3属于组织R3，于C2通道运行。\nP1、P2和P3分别是组织R1、R2和R3的节点。\n排序节点由O4提供，属于组织R4.\n搭建过程 # 与真正的商业应用场景相比，角色和商业和逻辑都很简化，但很适合用来理解不同节点和角色之间的功能和交互。接下来，我将一步一步说明网络的搭建过程。\n创建网络并添加网络管理员\n每一个组织需要通过MSP中的 CA 机构颁发的证书才能加入网络，因此，每个节点都需要有相应的 CA。\nR4作为网络启动者，需要先配置网络并设立O4排序节点！网络创建后，添加R1作为网络管理员，因此，R1和R4可以对网络进行配置（NC4）。\n定义联盟并创建通道\nR1和R2将通过C1进行业务交互，因此需要在网络中定义联盟，因为现在R1和R4都可以对网络进行配置，因此都可以定义联盟。\n接着为这个联盟创建通道C1（连接至排序服务O4）。\n加入节点、部署智能合约与应用\nP1节点加入已经建立的通道C1，维护着一个账本L1。\n这时候就可以在节点上安装和实例化智能合约了。Fabric的智能合约是链码，把链码存储在节点的文件系统上称为安装智能合约，安装后还需要在特定的通道上启动和实例化链码，至此，应用可以发送交易 proposal 至背书节点了（需要遵守链码设置的背书策略）。\n如下图所示，P1节点安装链码S5并在通道C1实例化后，就可以响应来自应用A1的链码调用了;P2节点安装链码S5并在通道C1实例化后，就可以响应来自应用A2的链码调用了。\n通道中的每一个节点都是提交节点，可以接收新区块（来自排序节点）进行验证，并提交至账本；而部署了链码的一些节点则可以成为背书节点。\n定义新联盟、创建新通道\n在网络中定义新联盟并加入C2通道。\n加入新节点并部署智能合约与应用\n值得注意的是，有些节点会同时加入多个通道，在不同的业务中扮演不同的角色，其他流程同上。\n网络搭建完成\nFabric采用权限管理、通道等机制，并通过对不同节点功能分工，提升了系统的运行效率，并保障了复杂业务场景中的安全和隐私；强大的链码和可自定义的背书策略等也保障了系统的拓展性，可以处理复杂的业务逻辑。\nHyperledger Fabric 安全分析 # Fabric 安全机制 # Fabric设计了很多机制来保障系统的安全性。\n系统配置与成员管理 # 区别于比特币、以太坊等公链，加入Fabric网络需要进行权限验证，Fabric CA为成员管理使用X.509证书机制以保障其权限，避免潜在Spoofing攻击等。\n现有的系统成员需要制定加入新成员的规则，比如进行多数投票等；现有成员也需要决定网络和智能合约的更新和改变，这样能够很大程度上防止恶意节点破坏系统安全性；现有节点不能自行升级权限；除此之外，还需要决定系统的通用数据模型等设置。\nFabric的网络传输采用TLSv1.2，可以保障数据的安全性；且系统中的操作，如发起交易、背书等都会通过数字签名技术来记录，很容易追溯一些恶意操作。但值得注意的是，排序节点可以获取系统中所有节点的交易数据，因此，排序服务节点的设定对于整个系统的安全性尤其重要，它的公正性会很大程度影响整个系统的运作，甚至决定了整个系统是否值得信任，因此，需要根据业务和系统结构慎重选择。\n公链系统中，所有节点都有区块链账本的副本，并且执行智能合约；而在Fabric系统中，业务相关节点会形成节点组，存储与其交易（业务）相关的账本，而通过链码对账本的更新也会被限制在节点组的范围内，从而保障整个系统的稳定性。\n智能合约的执行称为交易，对于Fabric系统内的交易，也必须要保持其一致性，往往采用密码学技术来防止交易被篡改，如采用SHA256、ECDSA等检测修改；Fabric采取模块化、可插拔的设计，将交易的执行、验证共识进行分离，因此，可以采取不同的共识机制或规则，不仅能够根据需求选择不同的共识机制，更具拓展性，也能提高系统安全性。\n这些配置和规则共同决定了系统的安全性，需要在业务需求、效率和安全性上作权衡。\n智能合约安全 # Fabric的链码需要安装在节点上并且实例化，安装链码需要有 CA 的验证，因此要注意权限管理；启动后是运行在独立的 Docker 容器中的，更轻量级，但是因为它能够访问Fabric网络，如果没经过严格的代码审计以及对网络进行隔离，会造成一些恶意后果。\nFabric的链码可以用多种通用型的编程语言撰写，例如Go、Java等，这让系统有了更强的拓展性，也更容易接入现有系统和工具，但因为其执行结果是不缺性的，编程语言的一些特性（如随机数、系统时间戳、指针等）可能会造成不同背书节点执行结果不同，造成系统不一致性；此外，因为链码可以访问一些外部的 Web 服务、系统命令、文件系统和第三方库等，也会造成一些潜在的风险。因此，用这些通用语言开发的链码需要相对独立且加强代码审计，以避免一些因编程语言带来的安全风险。\n交易隐私 # Fabric采用了通道机制来划分整个系统为多个子区块链（账本），只有加入通道的节点才能查看和存储交易信息，但排序节点可以看到。\n那有什么办法在通道中保障一些私有数据的隐私呢？\nFabric提供了一种存储私有数据的方式，使通道中的节点可以选择特定的数据分享对象（节点）。\n在这种机制下，真实的数据会通过gossip协议发送到指定的节点，数据存放私有数据库中，只有授权节点可以通过链码进行访问，因为这个过程并没有涉及到排序服务，所以排序节点也无法获取。\n而在系统内传播、排序与写入账本的数据是经过哈希加密的版本，因此交易仍然可以被各个节点验证，但因为哈希的特性，可以有效保护原数据不被泄漏。\n但值得注意的是，如果在背书节点模拟交易过程中需要使用到数据，那需要采取额外的机制来保障数据对于背书节点的可读性和对其他节点的不可见性（如非对称加密等）。\n总结 # 以上就是对Hyperledger Fabric网络搭建和安全体系分析了，接下来将会开始学习Go和链码的开发，通过项目实战来对其进行深入了解学习！\n参考资料 # FITE3011 Distributed Ledger and Blockchain, Allen Au，HKU "},{"id":4,"href":"/zh/docs/hyperledger_fabric/blockchain_hyperledger_fabric_structure/","title":"Hyperledger Fabric 系统架构详解","section":"Docs","content":" Hyperledger Fabric 系统架构详解 # 前言 # 因为毕业 Case Study 的项目主要是基于Ethereum公链，也没有面向企业的应用场景，所以之前对Hyperledger Fabric的了解大多只是停留在它的权限管理机制、通道、灵活的智能合约编写等几个特色的概念，对它的架构、各个节点的角色、运行机制等都是一知半解。最近在上 HKU 的\u0026lt;FITE3011 Distributed Ledger and Blockchain\u0026gt;课程，教授对Hyperledger Fabric的工作原理、网络搭建及链码相关的知识做了很详细的讲解，受益匪浅，通过本文来梳理一下，如有错漏，欢迎交流指正。\nHyperledger 概述 # 要学习Hyperledger Fabric，先来看看它的母项目Hyperledger是什么。\n企业级应用有较复杂的业务逻辑和参与者角色划分，对于业务执行效率、安全性要求很高，并且针对常见的如支付、数据/信息交易等场景，隐私保护也是重中之重，因此，常见的比特币、以太坊等公链并不符合大部分企业应用需求。但是区块链的分布式、不可篡改的历史账本等特性在溯源、跨境电商等场景中又能够避免因各个国家/地区法律法规、货币等造成的复杂操作流程，大大提高效率。因此，针对企业的联盟链也在不断发展。\n联盟链严格意义上并不是真正的“去中心化”，它通过引入了权限管理机制（结合企业在现实业务中的角色）来弱化对节点作恶的预防机制，从而能提高效率、应对复杂的业务逻辑。\n其中，Hyperledger是由 Linux 基金会维护的一组专注于跨行业分布式技术的开源项目，旨在创建企业级、开源、分布式的分类框架和代码库来支持业务用例，提供中立、开放和社区驱动的基础设施；建立技术社区并推广，开发区块链和共享账本概念验证、使用案例、试验和部署；建立行业标准，鼓励更多企业参与到分布式账本技术的建设和应用中来，形成一个开放的生态体系；教育公众关于区块链科技的市场机会。\n设计理念 # Hyperledger有如下几个核心设计理念：\n它针对企业具体的业务场景提升效率，并且对溯源等场景有着独特优势，每个企业都可以针对自己的场景维护独立的Hyperledger项目，因此，它不需要像公链一样通过数字货币来激励用户参与区块链系统。 企业的应用场景较为复杂，往往 Hyperledger 只是在其中参与了某个或某些环节，因此与其他现有系统的交互必不可少，因此 Hyperledger 在设计上注重配备完整的 API 以供其他系统调用与交互。 Hyperledger的框架结构是模块化、可拓展，企业可以根据具体的业务需求选择不同的模块，避免复杂的业务逻辑和臃肿的系统。 企业应用的安全性是重中之重，尤其是许多应用场景牵扯到高价值交易或敏感数据，因此提供了很多机制来保障安全性（如Fabric的通道机制等） 除了与现有的系统交互外，企业未来的区块链应用中还可能会和很多不同的区块链网络进行交互，因此大部分智能合约/应用应该具备跨区块链网络的可移植性，以形成更复杂和强大的网络。 框架 # Hyperledger下有如下几个项目，其中Fabric目前应用最为广泛，本文也将主要介绍Fabric区块链网络\nBurrow Fabric Grid Indy Iroha Sawtooth 工具 # Hyperledger Cello。主要用于更方便地搭建和管理区块链服务，降低项目框架部署、维护的复杂度；可以用来搭建区块链 BaaS 平台；可以通过 Dashboard 来创建和管理区块链，技术人员可以更方便地进行开发和部署；可以将 SaaS 部署模型引入区块链系统，帮助企业进一步开发框架。 Hyperledger Explorer。是一个可视化区块链的操作工具，可以用于创建对用户友好的 Web 应用程序；是首个Hyperledger的区块链浏览器，用户可以查看/调用/部署/查询交易、网络、智能合约、存储等信息。 Hyperledger Fabric # 我们着重来讲讲其中应用最广泛的Fabric项目，它是由 Linux 基金会维护的一个模块化、可拓展的区块链联盟链项目，不依赖任何加密货币，它对有着共同目标（业务需求）但彼此不完全信息的实体之间的业务提供了保护，例如跨境电商、资金交易、溯源等。\n架构 # 在大部分公链中，架构为Order - Execute - Validate - Update State。如比特币区块链中，如果有一个新交易，会先采用 PoW 机制对 Block 进行排序，然后比特币网络中的每个节点逐个进行验证，最后更新状态。因为需要依序进行验证，这种方式决定了其执行效率相对较低。\n而Fabric采用了Execute - Order - Validate - Update State架构。\n收到一笔新的交易后，首先提交至背书节点本地模拟交易执行（并背书），再将已背书交易排序并广播，各个节点对交易进行验证后更新状态。\n正如上述联盟链特性中所述，Fabric网络的加入需要得到许可（身份验证），Fabric网路中的每个节点都有自己的身份。\n总的来说，Fabric通过模块化、可插拔的架构来支持企业的复杂业务场景，通过身份验证（绑定现实身份）来弱化节点作恶，使用通道机制大大提升了系统的安全性和隐私保护。\nMSP 成员服务提供商 # 那么，参与Fabric网络的身份是怎样管理的呢？\nFabric有一个 MSP(Membership Service Provider)成员管理提供商，它主要用来管理 CA 证书来验证哪些成员是可信任的。Fabric CA模块是独立的，可以管理证书服务，也可以允许第三方 CA 的接入，大大拓展的系统的应用范围。\n如上图所示，Fabric CA提供了客户端和 SDK 两种方式来和 CA 进行交互，每个Fabric CA都有一个根 CA 或中间 CA，为了进一步提高 CA 的安全性，可以采用集群来搭建中间 CA。\n更具体一点看 CA 的层级体系，一般是采用根 CA、业务 CA 和用户 CA 三层树结构，所有的下层 CA 会继承上层 CA 的信任体系。根 CA 用来签发业务 CA，业务 CA 用来签发具体的用户 CA（身份认证 CA、交易签名、安全通讯 CA 等）\n通道 # 上文提到Fabric用 Channel 通道机制来保障交易的安全和隐私性，本质上每一个通道就是一个独立的账本，也是一个独立的区块链，有着不同的世界状态，网络中的一个节点可以同时加入多个通道。这种机制可以很好地划分不同的业务场景，也不用担心交易信息泄漏问题。\n链码 # Fabric也有类似以太坊的智能合约，称为 Chaincode 链码，智能合约使外部的应用程序可以和Fabric网络中的账本进行交互。不同于Ethereum，Fabric使用 Docker 而不是特定的虚拟机来存放链码，提供了一个安全、轻便的语言执行环境。\n链码主要分成系统链码和用户链码两种，系统链码嵌入在系统内，提供对系统进行配置、管理的支持；而用户链码则是运行在单独的 Docker 容器中，提供对上层应用的支持，用户通过链码相关的 API 编写用户链码，即可对账本中状态进行更新操作。\n链码经过安装和实例化操作后即可被调用，在安装的时候需要指定具体安装到哪个 Peer 节点（有的节点可以没有链码），实例化时还需要指定通道及背书策略。\n链码之间也可以相互调用，从而创建更灵活的应用逻辑。\n共识机制 # Fabric中广义的共识机制包括背书、排序和验证三个环节，狭义的共识是指排序，\nFabric区块链网络中，不同参与者之间交易必须按照发生的顺序写到分布式账本中，依赖共识机制，主要有三种：\nSOLO（只限于开发） Kafka（一种消息平台） Raft（相比 Kafka 更中心化） 网络协议 # 那Fabric网络中各个节点的状态分发又是怎么进行的呢？\n外界的客户端是通过gRPC来对Fabric网络中的各个节点进行远程调用，而P2P网络中各个节点之间的同步是通过Gossip协议来进行的。\nGossip协议主要是用于网络中多个节点之间的数据交换，比较容易实现且容错率很高，原理就是数据发送一方从网络中随机选取若干个节点发送过去，等几个节点接收到这些数据后再随机发送给除了发送方外的若干节点，不断重复，最终所有节点达成一致（复杂度为 LogN）。\n分布式账本 # 最终所有的交易都会记录到分布式账本中，这也是区块链诸多特性的核心。Fabric中交易可以存储相关业务信息，区块是一组排列后的交易集合，将区块通过密码算法链接起来就是区块链。分布式账本主要记录世界状态（最新的分布式账本状态，一般使用CouchDB以方便查询）和事务日志（世界状态的更新历史，记录区块链结构，使用LevelDB），对账本的每个操作都会记录在日志中，不可篡改。\n应用编程接口 # 对于基于Fabric的应用，则主要提供了 SDK 开发工具包和 CLI 命令行两种方式进行交互。\nFabric 区块链核心角色 # 首先要提的是Fabric网络中的角色都是逻辑角色，比如 Peer 节点 A 可能既是排序节点，也可能在某些业务中是背书节点，而一个角色也不仅仅由单一节点担任。\n接下来介绍一下各个角色的作用和职能。\nClients 客户端主要给交易签名，提交交易 Proposal 给背书节点，接收已经背书后的交易广播给排序节点；背书节点则是本地模拟执行交易 Proposal 验证交易（策略由 Chaincode 制定），签名并返回已背书交易；排序节点则将交易打包为 block 然后广播至各个节点，不参与交易的执行和验证，多个排序节点可以组成 OSN；所有的节点都维护区块链账本。\n优势总结 # Fabric通过将企业应用的各个复杂环节分配到各个逻辑角色节点（背书、排序等），不需要所有节点都承担如排序这样资源消耗较大的操作，消除了网络瓶颈；分配了角色后某些交易只在特定的节点部署和执行，且可以并发执行，大大提升效率和安全性，也隐藏了一些商业逻辑；因此，可以根据不同的业务需要来形成多种灵活的分配方案，极大增强了系统的拓展性。\n将共识机制、权限管理、加密机制、账本等模块都设置为可插拔，且不同的链码可以设置不同的背书策略，信任机制更加灵活，这样可以根据业务需要设置自己的高效系统。\n成员身份管理的Fabric CA作为单独的项目，能够提供更多功能，也能够与很多第三方 CA 直接进行接入和交互，功能更强大，适合企业复杂的场景。\n多通道的特性是不同通道之间的数据彼此隔离，提高了安全性和隐私保护。\n链码支持如Java、Go、Node等不同的编程语言，更加灵活，也支持更多第三方拓展应用，降低了业务迁移和维护成本。\nFabric 应用开发及交互 # 上图就是作为一个区块链开发者在应用Fabric区块链中的开发和交互流程。\n开发者主要负责开发应用和智能合约（链码），应用通过 SDK 与智能合约进行交互，而智能合约的逻辑可以对账本进行get、put、delete等操作。\nFabric 工作流程 # 接下来通过一个完整的交易流来梳理一下Fabric网络的工作原理\n在所有操作之前，需要向 CA 获取合法身份并且指定通道 首先，Client 提交交易 Proposal（含自己的签名）至背书节点 背书节点接收到交易 Proposal 后用本地状态模拟执行，对交易进行背书、签名并返回（其中包含 Read-Write Set、签名等） Client 收集到足够的背书后（策略由 Chaincode 制定，如图中示例为得到 2 个背书）提交已背书交易至排序节点（OSN） 排序节点将交易打包成 blocks，排序（不执行或验证交易正确性）并广播至所有节点 所有节点对新 blocks 进行验证并提交至账本 接下来对每个环节进行一些详细的拆解\n执行/背书环节 # Client 提交交易 proposal 后，背书节点会首先核对 Client 的签名，用本地状态模拟执行，对交易进行签名和 Read-Write Set 回 Clients，R-W Sets 主要包含key, version, value三个属性，Read-Set 包含交易执行中读取的所有变量和其version，对账本进行 write 操作的话version会产生变化，Write-Set 包含所有被编辑的变量及其新值。\n背书节点在执行交易时值根据本地区块链的状态检查链码是否正确，执行并返回。\nFabric 支持多种背书策略，Client 在提交至排序节点前会验证是否满足背书要求，值得注意的是如果只做了查询账本操作，Client 不会提交至 OSN。\n上文所提到的交易 proposal 主要包括链码、链码的输入值、Client 的签名，而背书节点返回至 Client 的信息则包括返回值、模拟执行结果的 R-W Set 以及背书节点的签名，组合起来则是已背书节点。\n背书是相关组织对交易的认可，即相关节点对交易进行签名。对于一个链码交易来说，背书策略是在链码实例化的时候指定的，一笔有效交易必须是背书策略相关组织签名才能生效，本质上Fabric区块链中的交易验证是基于对背书节点的信任，这也是称Fabric并不是严格意义上的去中心化的原因之一。\n以下是一个简单的链码执行示例\nfunc (t *SimpleChaincode) InitLedger(ctx contractapi.TransactionContextInterface) error { var product = Product { Name: \u0026#34;Test Product\u0026#34;, Description: \u0026#34;Just a test product to make sure chaincode is running\u0026#34;, CreatedBy: \u0026#34;admin\u0026#34;, ProductId: \u0026#34;1\u0026#34; } productAsBytes, err := json.Marshal(product) err = ctx.GetStub().PutState(\u0026#34;1\u0026#34;, productAsBytes) if err != nil { return err } } 在这个简单示例中，链码的主要操作就是更新了key-value值，经过了这个操作后，version会变化。\n执行后返回的 R-W Set 为\nkey: 1 value: Product { Name: \u0026#34;Test Product\u0026#34;, Description: \u0026#34;Just a test product to make sure chaincode is running\u0026#34;, CreatedBy: \u0026#34;admin\u0026#34;, ProductId: \u0026#34;1\u0026#34; } 的Json形式 排序环节 # Client 提交已背书交易至排序节点（排序节点可通过一些共识策略组成 OSN），排序节点接收到交易后，会打包成 blocks 并按照配置中的规则进行排序，在此过程中，只执行排序操作，而不进行任何执行或验证，排序完成后发送至所有节点。\n排序服务用来对全网交易达成一致，只负责对交易顺序达成一致，避免了整个网络瓶颈，更容易横向拓展以提升网络效率，目前支持Kafka和Raft两种，Fabric区块链网络的统一/完整性依赖于排序节点的一致性。\nRaft 共识机制属于非拜占庭共识机制，使用了领导者和跟随者（\u0003Leader 和 Follower）模型，当一个 Leader 被选出，日志信息会从 Leader 向 Follower 单向复制，更容易管理，在设计上允许所有节点都可以称为 Orderer 节点，相比 Kafka 更中心化，其实也允许采用 PBFT 共识机制，但是性能往往很差。\n验证环节 # 当节点接收到由排序节点发送来的区块时，会对区块中的所有交易进行验证并标记是否可信，主要验证两个方面：1.是否满足背书策略。2.交易结构的合法性，是否有状态冲突，如 Read-Set 中的version是否一致等。\n总结 # 以上就是对Hyperledger Fabric架构的梳理了，虽然取舍了部分去中心化的理念，但是作为一个面向企业应用的开源联盟链，它鼓励了更多企业参与到分布式账本技术的建设和应用中来，现在国内也有很多联盟链的自研平台，如蚂蚁链、趣链等，相信未来会有更多企业参与到这个开放的生态体系！\n参考资料 # FITE3011 Distributed Ledger and Blockchain, Allen Au，HKU 企业级区块链实战教程，张应平 "},{"id":5,"href":"/zh/docs/ipfs/blockchain_ipfs_practice/","title":"IPFS 本地节点搭建（命令行）","section":"Docs","content":" IPFS 本地节点搭建（命令行） # 前言 # 上一篇《IPFS 分布式文件存储原理》对于 IPFS 系统的设计理念、功能、工作原理及 IPNS 做了详细的介绍，那么，如何在本地搭建一个 IPFS 节点呢？\n本文在macOS 11.2.3系统上搭建了一个 IPFS 节点（命令行版本），并对文件上传、下载、网络同步、pin、GC、IPNS等进行了实际操作，以加深对 IPFS 工作原理的理解。\n代码实践 # 安装 # wget https://dist.ipfs.io/go-ipfs/v0.8.0/go-ipfs_v0.8.0_darwin-amd64.tar.gz tar -xvzf go-ipfs_v0.8.0_darwin-amd64.tar.gz cd go-ipfs ./install.sh ipfs --version 启动 # # 启动节点 ipfs init # 上传文件 ipfs add ipfs_init_readme.png # 上传文件并且只输出哈希值 ipfs add -q ipfs_init_readme.png # 上传目录 ipfs add -r [Dir] # 查看文件 ipfs cat /ipfs/QmQPeNsJPyVWPFDVHb77w8G42Fvo15z4bG2X8D2GhfbSXc/readme ipfs cat /ipfs/QmQPeNsJPyVWPFDVHb77w8G42Fvo15z4bG2X8D2GhfbSXc/quick-start # 查看自己上传的文件 ipfs cat QmaP3QS6ZfBoEaUJZ3ZfRKoBm3GGuhQSnUWtkVCNc8ZLTj # 查看图片并输出到文件 ipfs cat QmfViXYw7GA296brLwid255ivDp1kmTiXJw1kmZVsg7DFH \u0026gt; ipfsTest.png # 下载文件 ipfs get QmfViXYw7GA296brLwid255ivDp1kmTiXJw1kmZVsg7DFH -o ipfsTest.png # 压缩并下载文件 ipfs get QmfViXYw7GA296brLwid255ivDp1kmTiXJw1kmZVsg7DFH -Cao ipfsTest.png 开启/加入服务 # # 查看当前节点信息 ipfs id # 查看IPFS配置信息 ipfs config show # 开启节点服务器 ipfs daemon API 服务，默认在 5001 端口，可以通过 http://localhost:5001/webui 进行访问\n网关服务，默认在 8080 端口，在浏览器里访问文件需要借助于 IPFS 提供的网关服务，由浏览器先访问到网关，网关去获取 IPFS 网络杀过了的文件。通过 http://localhost:8080/ipfs/[File Hash] 来访问上传到 IPFS 的文件\n文件操作 # # 列出文件 ipfs files ls # 创建目录 ipfs files mkdir # 删除文件 ipfs files rm # 拷贝文件 ipfs files cp [File Hash] /[Dest Dir] # 移动文件 ipfs files mv [File Hash] /[Dest Dir] # 状态 ipfs files stat # 读取 ipfs files read 使用 IPNS 来解决文件更新问题 # # 使用IPNS发布内容以自动更新 ipfs name publish [File Hash] # 查询节点id指向的Hash ipfs name resolve # 有多个站点需要更新，可以新产生一个秘钥对，使用新的key发布 ipfs key gen --type=rsa --size=2048 mykey ipfs name publish --key=mykey [File Hash] Pinning # 当我们向 IPFS 网络请求文件时，IPFS 会把内容先同步的本地提供服务，使用 Cache 机制处理文件以防止存储空间不断增长，如果文件一段时间未被使用则会被“回收”，Pining 的作用就是确保文件在本地不被“回收”。\n# pin一个文件 ipfs pin add [File Hash] # 查询某一个Hash是否被pin ipfs pin ls [File Hash] # 删除pin的状态 ipfs pin rm -r [File Hash] # GC操作 ipfs repo gc 总结 # 本文主要在本地部署了 IPFS 文件系统并对基本操作进行了尝试，基于macOS 11.2.3和go-ipfs_v0.8.0_darwin-amd64版本，不同系统操作可能会因版本或依赖问题不一样，如有错漏，欢迎交流指正。\n参考资料 # IPFS 官网 "},{"id":6,"href":"/zh/docs/ipfs/blockchain_ipfs_structure/","title":"IPFS 分布式存储协议分析与思考","section":"Docs","content":" IPFS 分布式存储协议分析与思考 # 前言 # 最近在做学校的 Case Study 项目，是一个基于Ethereum平台的音乐版权管理项目，其中对于音乐作品、版权证明文件等上传用到了 IPFS 分布式文件存储技术，主要是利用其去重的特性来检测侵权行为。对 IPFS 这个系统产生了兴趣，阅读了QTech 平台上的IPFS 系列文章，也查询了一些相关资料，通过本文梳理一下，如有错漏，欢迎交流指正。\n概述 # 我们日常使用网盘或其他服务时大多都是访问文件所在的特定的服务器（IP 地址），请求文件并下载到本地，通过的是 HTTP 协议，本质上是基于位置寻址的，访问 URL 来得到一层层找到具体的文件，这种方式固然便捷，但是存在一些问题。文件依托于特定的服务器，因此一旦中心化的服务器宕机或者文件被删除了，内容将永久丢失，并且如果离服务器很远/同时访问文件的人很多的话访问速度也会比较慢；而且同样一份文件可能重复存储在不同的服务器中，造成资源的浪费；此外就是存在严重的安全隐患，DDoS、XSS、CSRF 等攻击都可能对文件安全性造成威胁。\n那有没有更好的解决方案呢？\n试想我们把文件存储在一个分布式网络里，每个节点都可以存储文件，用户可以通过访问一个类似目录索引的方式来向最近的节点互相请求文件。这就是 IPFS 星际文件系统的解决思路，它是一个点对点的超媒体文件存储、索引、交换协议，由 Juan Benet 在 2014 年 5 月发起。\n特点 # IPFS 想把全世界所有部署了相同文件系统的计算设备链接在一起，构建一个分布式网络来替代传统中心化的服务器模式，每个节点都可以存储文件，用户通过DHT(Distributed Hash Table)分布式哈希表来获取文件，速度更快、更安全，网络安全性更强。\n因为通过 IPFS 存储的文件内容是通过分块求 Hash 值存储为地址的，本质上是通过多重哈希来确定文件的地址，这是一种去中心化但是基于内容寻址的方式，通过对数据本身进行加密，生成独一无二的 Hash 以供查找，这种方式下，即使是微小的改变，也会造成 Hash 结果截然不同，因此很容易能够从 Hash 检测内容是否被篡改，甚至不用访问文件本身。\n不同于传统的服务器模式，IPFS 是一个统一的网络，因此已经上传的相同内容的文件不会重复存储（可以通过 Hash 值检验），极大地节约了整体网络资源，也更加高效。而且理论上只要节点达到一定规模，文件将永久保存，且同一个文件可以从多个（也更近）的节点下载，通讯效率也会更高。\n除此之外，因为是分布式网络进行存储，也可以天然地避免传统 DDoS 等攻击。\n功能 # 除了文件存储外，IPFS 还有 DHT 组网、Bitswap 文件交换等功能，之后也会单独写博文进行讲解。\n工作原理 # 作为一个文件存储系统，上传文件和下载文件是两个最基本的操作，我们分别讲一下原理。\nIPFS add 命令 # 在 IPFS 系统中执行 add 操作就完成了上传操作，那是怎么上传的呢？\n在 IPFS 文件存储系统中，每当上传一个新文件，系统会将单个文件拆分成若干个 256KB 的 block，每个 block 会有一个专属的 CID 进行标识，这个后面会详细讲；然后计算每一个 block 的 Hash 值，并存储再一个数组中，最后对这个数组求 Hash 得到文件的最终 Hash 值；接着将文件的 Hash 和所有的 blocks Hash 的数组组成成一个对象，也就形成了一种索引结构；最后把文件 block 和这个索引结构全部上传到 IPFS 节点，同步到 IPFS 网络。\n文件上传时有两个值得注意的情况：1.文件特别小，如果文件小于 1KB 的话就不浪费一个 block 了，会直接和 Hash 一起上传到 IPFS。2.文件特别大，比如之前上传了一个 1G 的视频，之后又加了几 KB 的字幕文件，这种情况下未变化的 1G 部分是不会重新分配新的空间的，而只会为追加的字母文件部分分配新的 block，再重新上传 Hash。\n因此，很好理解的是，即使是不同文件的相同部分也只会存储一份，很多文件的索引会指向同一个 block，所形成的结构就是 MerkleDAG 数据结构。\n值得注意的是，当节点执行 add 操作时，会保留到本地 blockstore 中，但不会立刻主动上传到 IPFS 网络中，也就是说，与其连接的节点并不会存储这个文件，除非有某个节点请求过该 block 数据！因此，它并不是一个自动备份数据的分布式数据库。IPFS 这种设计是出于网络带宽、可靠性等方面的考虑。\n还有一个细节就是，当节点在执行add命令时，还会广播自己的块信息，并维护一个所有发给这个节点的 block 请求列表，一旦 add 命令添加到数据满足这个列表，就会主动向对应的节点发送数据并且更新列表。\nIPFS get 命令 # 那文件上传后，要怎么查找访问呢？\n这就关系到上文所提到的 IPFS 索引结构是DHT（分布式哈希表），通过对DHT进行访问可以很快访问得到数据。\n那如果想要查找一个本地没有的数据呢？\n在 IPFS 系统中，所有和当前节点连接的节点会构成一个 swarm 网络，当节点发送一个文件请求(即get)时，首先会在本地的 blockstore 里查找请求的数据，如果没找到的话，就会向 swarm 网络发出一个请求，通过网络中的DHT Routing找到拥有该数据的节点。\n怎么知道网络中哪个（哪些）节点拥有这个请求文件呢？\n如上文add命令所讲的那样，当一个节点加入到 IPFS 网络中后，会告诉其它节点自己存储了什么内容（通过广播DHT），这样每当有用户希望检索的内容正好在这个节点上时，其它节点就会告诉用户要从这个节点索取他想要的内容。\n一旦找到拥有这个数据的节点，就会把请求数据反馈回来，这样本地节点会把收到的 block 数据缓存一份到本地的 blockstore 中，这样整个网络中相当于多了一份原数据的拷贝，更多节点请求数据的话，查找就变得更容易，因此数据的不可丢失性也是基于这个原理，只要有一个节点保存着这个数据，就可以被全网获取。\n在项目中，上传的文件可以通过ipfs.io网关直接获取到文件，类似于https://ipfs.io/ipfs/Qm.....这样的网站地址，这个是什么原理呢？\nipfs.io网关实际上就是一个 IPFS 节点，当我们打开上述这个网络链接的时候，实际上就是向这个节点发送了一次请求，因此ipfs.io网关会帮我们去向拥有这个数据的节点请求这个 block（如果这个文件是自己刚在本地节点通过add命令添加的话就会通过这种方式被上传到 IPFS 网络上），在swarm网络中通过DHT Routing获取到数据后，网关会自己先缓存一份，然后将数据通过 HTTP 协议发给我们，因此，就可以在浏览器直接看到这个文件啦！\n而任何其他机器通过浏览器访问这个链接时，因为ipfs.io网关已经缓存了这个文件，再次请求的时候，就不需要向原节点来请求数据了，可以直接从缓存中返回数据给浏览器。\n内容标识符 CID(Content-ID) # 现在考虑另一个问题，我们常见的图像为.jpg、.png，而常见的视频则是.mp4一样，可以直接从后缀名判断文件类型。通过 IPFS 上传的文件也可以是多种类型，也包含了很多信息，怎么进行分辨呢？\nIPFS 早期主要使用base58btc对multihash进行编码，但是在开发 IPLD（主要用来定义数据，给数据建模）的过程中会遇到很多与格式相关的问题，因此使用了一种叫CID的文件寻址格式来对不同格式的数据进行管理，官方的定义为：\nCID是一种自描述式的内容寻址的识别符，必须使用加密散列函数来得到内容的地址\n简单来说，CID通过一些机制来对文件所包含的内容进行自描述，包含了版本信息、格式等。\nCID 结构 # 目前CID有v0和v1两种版本，v1版本的CID由V1Builder生成\n\u0026lt;cidv1\u0026gt; ::= \u0026lt;mb\u0026gt;\u0026lt;version\u0026gt;\u0026lt;mcp\u0026gt;\u0026lt;mh\u0026gt; # or, expanded: \u0026lt;cidv1\u0026gt; ::= \u0026lt;multibase-prefix\u0026gt;\u0026lt;cid-version\u0026gt;\u0026lt;multicodec-packed-content-type\u0026gt;\u0026lt;multihash-content-address\u0026gt; 如上面列举的代码所示，采用的机制叫multipleformats，主要包括：multibase-prefix表示CID编码成字符串，cid-version表示版本变量，multicodec-packed-content-type表示内容的类型和格式（类似于后缀，但是作为标识符的一部分，支持的格式有限，且用户是不能随意修改的），multihash-content-address表示哈希值（让CID可以使用不同的 Hash 函数）。\n目前CID支持的multicodec-packed编码有原生的protobuf格式、IPLD CBOR格式、git、比特币和以太坊对象等格式，也在逐步开发支持更多格式。\nCID代码详解：\ntype Cid struct {str string} type V0Builder struct {} type V1Builder struct {} Codec uint64 MhType uint64 MhLength int // Default: -1 Codec表示内容的编码类型，如DagProtobuf, DagCBOR等，MhType表示哈希算法，如SHA2_256, SHA2_512, SHA3_256, SHA3_512等，而MhLength则表示生成哈希的长度。\n而v0版本的CID由V0Builder生成，以Qm字符串开头，向后兼容，multibase一直为base58btc，multicodec一直为protobuf-mdag，cid-version一直为cidv0，multihash表示为cidv0 ::= \u0026lt;multihash-content-address\u0026gt;。\n设计理念 # 通过CID这种二进制的特性，大大提高了对于文件 Hash 的压缩效率，因此可以直接作为 URL 的一部分进行访问；通过multibase的编码形式（如base58btc）缩短了CID的长度，这样更容易传输；可以表示任意格式、任何哈希函数的结果，十分灵活；可以通过结构中cid-version参数进行编码版本的升级；不受限于历史内容。\nIPNS # 如上文所述，IPFS 中文件内容的改变会造成其哈希值的变化，在实际应用中，如果通过 IPFS 托管网站等需要版本更新迭代的应用，每一次都通过更新后的 Hash 访问很不方便，因此，需要一个映射方案以保证用户体验，这样用户在访问时仅需要访问一个固定地址。\nIPNS(Inter-Planetary Naming System)就提供了这样的服务，它提供了一个被私钥限定的哈希 ID（通常是 PeerID）来指向具体的 IPFS 文件，文件更新后会自动更新哈希 ID 的指向。\n即使哈希值可以固定不变了，但是依然不便于记忆和输入，因此，有了更进一步的解决方案。\nIPNS 同样兼容 DNS，可以使用DNS TXT记录域名对应的 IPNS 哈希 ID，就可以域名来替换 IPNS 哈希 ID 来进行访问，从而实现更容易读写和记忆。\n总结 # 以上就是对 IPFS 分布式存储原理的梳理，它的组件、存储流程细节、GC 机制、数据交换模块 Bitswap、网络以及实际应用场景都有很多值得深入挖掘的部分。\n推荐阅读：趣链科技 QTech 平台《IPFS 系列文章》\n参考资料 # IPFS 官网 原来 IPFS 是这样存储文件的，QTech，趣链科技 IPFS 到底怎么工作的？，知辉 站在 Web3.0 理解 IPFS 是什么，Tiny 熊，登链社区 IPFS CID 研究，Sophie Huang "},{"id":7,"href":"/zh/docs/solidity/learn_solidity_from_scratch_ethersjs/","title":"Solidity 合约开发 - 玩转 ethers.js","section":"Docs","content":" Solidity 智能合约开发 - 玩转 ethers.js # 前言 # 在之前的《Solidity 智能合约开发 - 基础》中，我们学习了 Solidity 的基本语法，并且了解了可以通过 Brownie 与 HardHat 等框架进行调试。而另一篇《Solidity 智能合约开发 - 玩转 Web3.py》中我们也通过 Web3.py 直接与我们本地的 Ganache 节点进行交互了。\n原本因为之前比较熟悉 Python 的使用，所以想使用 Brownie 框架进行后续开发。然而经过了一番调研，业界还是使用 HardHat 框架居多，也有更多拓展，且我关注的 Solidity 教程也更新了 Javascript 版本，于是还是打算学习一下。\n为了更好了解其原理，也为我们后续更好使用框架打好基础，我们这次通过 ethers.js 来与我们部署在 Alchemy 平台上的 Rinkeby 测试网络进行交互。实现了基础的合约编译、部署至 Rinkeby 网络、与合约交互等功能。\n可以点击这里访问本测试 Demo 代码仓库。\nethers.js # ethers.js 是 Javascript 的一个开源库，可以与以太坊网络进行交互，其 GitHub 地址为 ethers.io/ethers.js，可以访问其官方文档进行使用。\n安装 # 我们可以通过 yarn 安装 ethers.js，如下：\nyarn add ethers 使用 # 使用 require 导入库即可使用\nconst ethers = require(\u0026#39;ethers\u0026#39;); Solidity 合约编译 # 合约源码 # // SPDX-License-Identifier: MIT pragma solidity ^0.8.7; contract SimpleStorage { uint256 favoriteNumber; bool favoriteBool; struct People { uint256 favoriteNumber; string name; } People public person = People({favoriteNumber: 2, name: \u0026#34;Arthur\u0026#34;}); People[] public people; mapping(string =\u0026gt; uint256) public nameToFavoriteNumber; function store(uint256 _favoriteNumber) public returns (uint256) { favoriteNumber = _favoriteNumber; return favoriteNumber; } function retrieve() public view returns (uint256) { return favoriteNumber; } function addPerson(string memory _name, uint256 _favoriteNumber) public { people.push(People({favoriteNumber: _favoriteNumber, name: _name})); nameToFavoriteNumber[_name] = _favoriteNumber; } } 这是一个简单的存储合约，通过一个 People 结构体对象来存储人名和他喜欢数字，通过一个数组来存储多个人的信息，并提供了添加、查找方法。\n读取合约源文件 # 当我们通过 VSCode 或其他编辑器完成 Solidity 合约编写与语法检查后，需要编译合约为 abi 文件与 bytecode。\n我们可以通过 yarn 安装 solc 命令行工具进行编辑，并且可以选择对应版本，命令如下：\nyarn add solc@0.8.7-fixed 安装完成后，，我们可以通过 solcjs 命令来进行编译，命令如下：\nyarn solcjs --bin --abi --include-path node_modules/ --base-path . -o . SimpleStorage.sol 因为编译合约是一个高频操作，我们可以在 package.json 中配置 compile 脚本命令，如下：\n\u0026#34;scripts\u0026#34;: { \u0026#34;compile\u0026#34;: \u0026#34;yarn solcjs --bin --abi --include-path node_modules/ --base-path . -o . SimpleStorage.sol\u0026#34; } 之后仅需执行 yarn compile 即可生成合约编译文件。\n获取编译结果 # 编译完成后会生成 abi 和 bytecode 文件，分别以 .bin 和 .abi 为后缀。\n获取 bytecode 与 abi # Solidity 合约的部署与交互需要 bytecode 与 abi 两个部分，我们可以通过通过以下代码将其写入对应变量供后续操作使用。\nconst fs = require(\u0026#39;fs-extra\u0026#39;); const abi = fs.readFileSync(\u0026#34;./SimpleStorage_sol_SimpleStorage.abi\u0026#34;, \u0026#34;utf-8\u0026#34;); const binary = fs.readFileSync(\u0026#34;./SimpleStorage_sol_SimpleStorage.bin\u0026#34;, \u0026#34;utf-8\u0026#34;); 创建 Rinkeby 测试网络环境（Alchemy） # 智能合约的调试需要将合约部署到实际的链上，我们选择部署到 Alchemy 平台的 Rinkeby 测试网进行后续调试开发，\nAlchemy 平台 # 首先我们访问 Alchemy 官网，注册并登录，会看到其 Dashboard，会展示所有已创建的应用。\n安装完成后选择 Create App 即可快速创建一个 Rinkeby 测试网络节点。\n创建完成后，点击 View Details，可以看到我们刚创建的 App 详细信息，点击右上角 View Key，可以查询我们的节点信息，我们需要记录下 HTTP URL，供后续连接使用。\n创建 Rinkeby 测试账户（MetaMask） # MetaMask # 完成了 Rinkeby 测试网络环境的创建，我们需要通过 MetaMask 创建账户，获取一些测试 Token，并且将账户私钥记录下来，以便后续使用。\n获取测试 Token # 创建账户后，我们需要一些测试 Token 来进行后续开发调试，我们可以通过以下网址获取：\nhttps://faucets.chain.link https://rinkebyfaucet.com/ 连接测试节点与钱包 # 连接节点 # ethers.js 提供了库可以方便地连接到我们的测试节点，其中 process.env.ALCHEMY_RPC_URL 为我们在 Alchemy 平台创建 App 的 HTTP URL：\nconst ethers = require(\u0026#39;ethers\u0026#39;); const provider = new ethers.providers.JsonRpcProvider(process.env.ALCHEMY_RPC_URL); 连接钱包 # ethers.js 也提供了方法可以连接到我们的测试钱包，其中 process.env.RINKEBY_PRIVATE_KEY 为我们从 MetaMask 复制的私钥。\nconst ethers = require(\u0026#39;ethers\u0026#39;); const wallet = new ethers.Wallet( process.env.RINKEBY_PRIVATE_KEY, provider ); Solidity 合约部署 # 创建合约 # 我们可以通过 ethers.js 库创建合约。\nconst contractFactory = new ethers.ContractFactory(abi, binary, wallet); 部署合约 # 下面我们介绍一下如何通过 ethers.js 库部署合约，其中 SimpleStorage 合约的 ABI 和 BIN 文件已经在上面的代码中读取过了。\n创建合约 # const contractFactory = new ethers.ContractFactory(abi, binary, wallet); 部署合约 # const contract = await contractFactory.deploy(); await contract.deployTransaction.wait(1); 与合约交互 # 我们也可以通过 ethers.js 来与合约进行交互。\nretrieve() # const currentFavoriteNumber = await contract.retrieve(); store() # const transactionResponse = await contract.store(\u0026#34;7\u0026#34;) const transactionReceipt = await transactionResponse.wait(1); 从 raw data 构造交易 # 除了直接调用部署合约方法等，我们也可以自己构造交易。\n构造交易 # const nonce = await wallet.getTransactionCount(); const tx = { nonce: nonce, gasPrice: 20000000000, gasLimit: 1000000, to: null, value: 0, data: \u0026#34;0x\u0026#34; + binary, chainId: 1337, }; 签名交易 # const signedTx = await wallet.signTransaction(tx); 发送交易 # const sentTxResponse = await wallet.sendTransaction(tx); await sentTxResponse.wait(1); 总结 # 以上就是我们通过 ethers.js 库与 Alchemy 的 Rinkeby 测试网络进行交互的步骤，在真正的生产项目开发中我们一般不会直接使用 ethers.js 这样的库，而是会使用 Brownie、HardHat 这样进一步封装的框架，但了解 Web3.py 或 ethers.js 等库的使用方法也非常重要。后续我还会对 HardHat 框架的使用作进一步讲解。\n参考资料 # Solidity 智能合约开发 - 基础 Solidity 智能合约开发 - 玩转 Web3.py Solidity, Blockchain, and Smart Contract - Javascript 版本 ethers.js 项目仓库 ethers.js 官方文档 Alchemy 官网 "},{"id":8,"href":"/zh/docs/solidity/learn_solidity_from_scratch_hardhat/","title":"Solidity 智能合约开发 - Hardhat 框架使用","section":"Docs","content":" Solidity 智能合约开发 - Hardhat 框架使用 # 前言 # 经过了前几篇对智能合约基础、Web3.py、ethers.js 的学习，我们已经掌握了通过程序与区块链网络直接交互的基础知识，不熟悉的同学可以回顾一下：\nSolidity 智能合约开发 - 基础 Solidity 智能合约开发 - 玩转 Web3.py Solidity 智能合约开发 - 玩转 ethers.js 但是在真正的复杂业务场景中，我们往往会使用一些进一步封装的框架，如 HardHat、Brownie、Truffle 等，HardHat 是其中应用最广泛、插件拓展最为强大的。本系列将从这篇开始专注于 Hardhat 框架的使用与最佳实践，而本篇则会通过一个简单的例子完成其安装、配置与使用。\n本文是对 Patrick Collins 的 『Learn Blockchain, Solidity, and Full Stack Web3 Development with JavaScript』 教程的学习整理，强烈建议看原教程视频了解更多细节。\n可以点击这里访问本测试 Demo 代码仓库。\nHardhat 介绍 # Hardhat 是一个基于 JavaScript 的智能合约开发环境，可以用于灵活地编译、部署、测试和调试基于 EVM 的智能合约，并且提供了一系列工具链来整合代码与外部工具，还提供了丰富的插件生态，提升开发效率。此外，它还提供了模拟以太坊的本地 Hardhat 网络节点，提供强大的本地调试功能。\n其 GitHub 地址为 NomicFoundation/hardhat，可以访问其官方文档了解更多。\nHardhat 使用 # 初始化项目 # 从零开始搭建一个 Hardhat 项目，我们需要预先安装好 node.js 与 yarn 环境，这部份参照官方说明根据自己的系统环境按照即可。\n首先，我们需要初始化项目并安装 hardhat 依赖包。\nyarn init yarn add --dev hardhat 初始化 Hardhat # 然后需要运行 yarn hardhat，通过交互式命令来进行初始化，根据项目需要进行配置，我们的测试 Demo 选择默认值。\n优化代码格式化 # VS Code 配置 # 我本地是通过 VS Code 进行代码开发的，可以通过安装 Solidity + Hardhat 与 Prettier 两个插件来进行代码格式化，可以使用打开 VS Code 设置，在 settings.json 中增加如下格式化配置：\n{ //... \u0026#34;[solidity]\u0026#34;: { \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;NomicFoundation.hardhat-solidity\u0026#34; }, \u0026#34;[javascript]\u0026#34;: { \u0026#34;editor.defaultFormatter\u0026#34;: \u0026#34;esbenp.prettier-vscode\u0026#34;, } } 项目配置 # 为了统一各个使用项目的开发人员的代码格式化样式，我们还可以为项目配置 prettier 与 prettier-plugin-solidity 插件支持：\nyarn add --dev prettier prettier-plugin-solidity 添加依赖后，可以在项目目录增加 .prettierrc 与 .prettierignore 配置文件来进行格式化统一：\n我的 .prettierrc 配置为：\n{ \u0026#34;tabWidth\u0026#34;: 4, \u0026#34;useTabs\u0026#34;: false, \u0026#34;semi\u0026#34;: false, \u0026#34;singleQuote\u0026#34;: false } 我的 .prettierignore 配置为：\nnode_modules package.json img artifacts cache coverage .env .* README.md coverage.json 编译合约 # 无需像 ethers.js 一样自定义 compile 命令，HardHat 预置了 compile 命令，可以将合约放在 contracts 目录下，然后通过 yarn hardhat compile 命令来编译合约：\n添加 dotenv 支持 # 在开始编写部署脚本之前，我们先配置一下 dotenv 插件，这样我们就可以使用 dotenv 来获取环境变量。我们在开发过程中，会牵扯到很多隐私信息，如私钥等，我们会希望将其存储在 .env 文件或直接设置在终端中，比如我们的 RINKEBY_PRIVATE_TOKEN，这样我们就可以在部署脚本中使用 process.env.RINKEBY_PRIVATE_TOKEN 获取到值，无需在代码中显式写入，减少隐私泄漏风险。\n安装 dotenv # yarn add --dev dotenv 设置环境变量 # 在 .env 文件中，我们可以设置环境变量，比如：\nRINKEBY_RPC_URL=url RINKEBY_PRIVATE_KEY=0xkey ETHERSCAN_API_KEY=key COINMARKETCAP_API_KEY=key 我们就可以在 hardhat.config.js 中读取环境变量了：\nrequire(\u0026#34;dotenv\u0026#34;).config() const RINKEBY_RPC_URL = process.env.RINKEBY_RPC_URL || \u0026#34;https://eth-rinkeby/example\u0026#34; const RINKEBY_PRIVATE_KEY = process.env.RINKEBY_PRIVATE_KEY || \u0026#34;0xkey\u0026#34; const ETHERSCAN_API_KEY = process.env.ETHERSCAN_API_KEY || \u0026#34;key\u0026#34; const COINMARKETCAP_API_KEY = process.env.COINMARKETCAP_API_KEY || \u0026#34;key\u0026#34; 配置网络环境 # 往往我们的合约需要运行在不同的区块链网络上，如本地测试、开发、上线环境等等，HardHat 也提供了便捷的方式来配置网络环境。\n启动网络 # 我们可以直接运行脚本来启动一个 Hardhat 自带的网络，但该网络仅仅存活于脚本运行期间，想要启动一个本地可持续的网络，需要运行 yarn hardhat node 命令：\n执行完成后，就生成了测试网络与测试账户，供后续开发调试使用。\n我们还可以通过 Alchemy 或 Infura 等平台生成自己的测试网节点，记录其 RPC_URL 供程序连接使用。\n定义网络 # 完成网络环境准备后，我们可以在项目配置 hardhat.config.js 中定义网络：\nconst RINKEBY_RPC_URL = process.env.RINKEBY_RPC_URL || \u0026#34;https://eth-rinkeby/example\u0026#34; const RINKEBY_PRIVATE_KEY = process.env.RINKEBY_PRIVATE_KEY || \u0026#34;0xkey\u0026#34; module.exports = { defaultNetwork: \u0026#34;hardhat\u0026#34;, networks: { locakhost: { url: \u0026#34;http://localhost:8545\u0026#34;, chainId: 31337, }, rinkeby: { url: RINKEBY_RPC_URL, accounts: [RINKEBY_PRIVATE_KEY], chainId: 4, }, }, // ..., } 脚本 # 在 Hardhat 项目中，我们可以通过在 scripts 目录中编写脚本来实现部署等功能，并且通过便捷的命令执行脚本。\n编写部署脚本 # 接下来我们开始编写 deploy.js 脚本。\n首先，我们需要从 hardhat 中导入必要包：\nconst { ethers, run, network } = require(\u0026#34;hardhat\u0026#34;) 接着则编写 main 方法，包含我们的部署核心逻辑：\nasync function main() { const SimpleStorageFactory = await ethers.getContractFactory( \u0026#34;SimpleStorage\u0026#34; ) console.log(\u0026#34;Deploying SimpleStorage Contract...\u0026#34;) const simpleStorage = await SimpleStorageFactory.deploy() await simpleStorage.deployed() console.log(\u0026#34;SimpleStorage Contract deployed at:\u0026#34;, simpleStorage.address) // 获取当前值 const currentValue = await simpleStorage.retrieve() console.log(\u0026#34;Current value:\u0026#34;, currentValue) // 设置值 const transactionResponse = await simpleStorage.store(7) await transactionResponse.wait(1) // 获取更新后的值 const updatedValue = await await simpleStorage.retrieve() console.log(\u0026#34;Updated value:\u0026#34;, updatedValue) } 最后运行我们的 main 方法：\nmain() .then(() =\u0026gt; process.exit(0)) .catch((error) =\u0026gt; { console.error(error) process.exit(1) }) 运行脚本 # 完成脚本编写后，可以通过 Hardhat 提供的 run 命令来运行脚本。\n如不加网络参数，则默认使用 hardhat 网络，可以通过 --network 参数指定网络：\nyarn hardhat run scripts/deploy.js --network rinkeby 增加 etherscan 合约验证支持 # 将合约部署至 Rinkeby 测试网络后可在 Etherscan 上查看合约的地址，并且进行验证。我们可以通过网站进行操作，但 Hardhat 提供了插件支持，更方便进行验证操作。\n安装 hardhat-etherscan 插件 # 我们通过 yarn add --dev @nomiclabs/hardhat-etherscan 命令安装插件。\n启用 etherscan 合约验证支持 # 完成安装后，我们需要在 hardhat.config.js 中进行配置：\nrequire(\u0026#34;@nomiclabs/hardhat-etherscan\u0026#34;) module.exports = { // ..., etherscan: { apiKey: ETHERSCAN_API_KEY, }, // ..., } 定义 verify 方法 # 接下来我们需要在部署脚本 deploy.js 中添加 verify 方法。\nconst { ethers, run, network } = require(\u0026#34;hardhat\u0026#34;) async function verify(contractAddress, args) { console.log(\u0026#34;Verifying SimpleStorage Contract...\u0026#34;) try { await run(\u0026#34;verify:verify\u0026#34;, { address: contractAddress, constructorArguements: args, }) } catch (e) { if (e.message.toLowerCase().includes(\u0026#34;already verified!\u0026#34;)) { console.log(\u0026#34;Already Verified!\u0026#34;) } else { console.log(e) } } } 这个方法我们调用了 hardhat 包中的 run 方法，并且传递了一个 verify 命令，并且传递了一个参数 { address: contractAddress, constructorArguements: args }。因为可能我们的合约已经在 Etherscan 上验证过，所以我们做了一个 try...catch... 错误处理，如果验证过，则会抛出一个错误，并且输出一个提示信息，而不影响我们的部署流程。\n设置部署后调用 # 定义好我们的 verify 方法后，我们可以在部署脚本中调用它：\nasync function main() { //... if (network.config.chainId === 4 \u0026amp;\u0026amp; process.env.ETHERSCAN_API_KEY) { await simpleStorage.deployTransaction.wait(6) await verify(simpleStorage.address, []) } // ... } 在这里我们做了两个特殊处理。\n首先，我们仅需要在 rinkeby 网络上验证合约，而不需要在本地或其他网络环境验证，因此，我们对 network.config.chainId 进行判断，如果是 4，则执行验证操作；否则，不执行验证操作，此外仅在有 ETHERSCAN_API_KEY 环境变量时执行验证操作。\n另外，Etherscan 可能需要在部署后一段时间才能获取到合约地址，因此我们配置了 .wait(6) 等待 6 个区块后再进行验证。\n执行效果如下：\n我们通过 Etherscan 验证后访问后可以直接查看合约源码并进行交互。\n合约测试 # 对于智能合约来说，其大多数操作都需要部署上链，与资产交互，消耗 gas，且一旦有安全隐患会造成严重的后果。因此，我们需要对智能合约进行详细的测试。\nHardhat 提供了完备的测试调试工具，可以在 tests 目录中编写测试脚本，通过 yarn hardhat test 命令运行测试。\n编写测试脚本 # 为我们的部署脚本编写 test-deploy.js 测试程序，首先需要导入必要包：\nconst { assert } = require(\u0026#34;chai\u0026#34;) const { ethers } = require(\u0026#34;hardhat\u0026#34;) 然后编写测试逻辑：\ndescribe(\u0026#34;SimpleStorage\u0026#34;, () =\u0026gt; { let simpleStorageFactory, simpleStorage beforeEach(async () =\u0026gt; { simpleStorageFactory = await ethers.getContractFactory(\u0026#34;SimpleStorage\u0026#34;) simpleStorage = await simpleStorageFactory.deploy() }) it(\u0026#34;Should start with a favorite number of 0\u0026#34;, async () =\u0026gt; { const currentValue = await simpleStorage.retrieve() const expectedValue = \u0026#34;0\u0026#34; assert.equal(currentValue.toString(), expectedValue) // expect(currentValue.toString()).to.equal(expectedValue) }) it(\u0026#34;Should update when we call store\u0026#34;, async () =\u0026gt; { const expectedValue = \u0026#34;7\u0026#34; const transactionRespense = await simpleStorage.store(expectedValue) await transactionRespense.wait(1) const currentValue = await simpleStorage.retrieve() assert.equal(currentValue.toString(), expectedValue) // expect(currentValue.toString()).to.equal(expectedValue) }) 在 Hardhat 的测试脚本中，我们使用 describe 包裹测试类，并且使用 it 包裹测试方法。我们需要保证测试前合约已经部署，因此，我们通过 beforeEach 方法在每个测试方法执行前都会调用 simpleStorageFactory.deploy()，并且将返回的 simpleStorage 对象赋值给 simpleStorage 变量。\n我们使用 assert.equal(currentValue.toString(), expectedValue) 来对执行结果与预期结果进行比照，可以用 expect(currentValue.toString()).to.equal(expectedValue) 替代，效果一样。\n此外，我们还可以通过 it.only() 来指定仅执行其中一个测试方法。\n执行测试脚本 # 我们通过 yarn hardhat test 运行测试，且可以通过 yarn hardhat test --grep store 来指定测试方法。\n添加 gas-reporter 支持 # 如上文所述，gas 是我们在开发过程中需要特别关注的资源，尤其在 Ethereum 主网上尤其昂贵。因此，我们需要在测试过程中查看 gas 消耗情况。HardHat 也有一个 gas-reporter 插件，可以很方便地输出 gas 消耗情况。\n安装 gas-reporter 插件 # 我们通过 yarn add --dev hardhat-gas-reporter 命令来安装插件：\n启用 gas-reporter 支持 # 我们通过在 hardhat.config.js 中添加 gasReporter: true 及额外配置项来启用插件：\nrequire(\u0026#34;hardhat-gas-reporter\u0026#34;) const COINMARKETCAP_API_KEY = process.env.COINMARKETCAP_API_KEY || \u0026#34;key\u0026#34; module.exports = { // ..., gasReporter: { enabled: true, outputFile: \u0026#34;gas-reporter.txt\u0026#34;, noColors: true, currency: \u0026#34;USD\u0026#34;, coinmarketcap: COINMARKETCAP_API_KEY, token: \u0026#34;MATIC\u0026#34;, }, } 我们可以指定输出文件、是否开启颜色、指定币种、指定代币名称，以及指定代币的 CoinMarketCap API 密钥来根据项目进一步控制输出。\n按照以上配置，运行 yarn hardhat test 输出效果如下：\n添加 solidity-coverage 支持 # 合约测试对于保障业务逻辑正确性与安全防范至关重要，因此，我们需要对合约进行覆盖率测试。HardHat 也有一个 solidity-coverage 插件，可以很方便地输出覆盖率情况。\n安装 solidity-coverage 插件 # 我们通过 yarn add --dev solidity-coverage 命令来安装插件：\n启用 solidity-coverage 支持 # 我们仅需在 hardhat.config.js 中导入包即可添加覆盖率测试支持：\nrequire(\u0026#34;solidity-coverage\u0026#34;) 运行覆盖率测试 # 通过 yarn hardhat coverage 即可运行覆盖率测试：\nTask # 上文我们对 hardhat 库的基础功能与脚本进行了一些使用。除此之外，我们还可以自定义一些任务供开发调试使用。\n编写 Task # Hardhat 中，我们将任务定义在 tasks 目录下，我们将编写一个 block-number.js 的 Task 来获取区块高度：\nconst { task } = require(\u0026#34;hardhat/config\u0026#34;) task(\u0026#34;block-number\u0026#34;, \u0026#34;Prints the current block number\u0026#34;).setAction( async (taskArgs, hre) =\u0026gt; { const blockNumber = await hre.ethers.provider.getBlockNumber() console.log(`Current Block Number: ${blockNumber}`) } ) Task 通过 task() 方法来创建，并通过 setAction() 方法来设置任务的执行函数。其中，taskArgs 是一个包含所有参数的对象，hre 是一个 HardhatRuntimeEnvironment 对象，可以用来获取其他的资源。\n在 hardhat.config.js 中导入task文件：\nrequire(\u0026#34;./tasks/block-number\u0026#34;) 运行 Task # 定义完成后，在项目命令的 AVAILABLE TASKS 中就有了我们刚定义好的 block-number 任务，可以通过 yarn hardhat block-number 命令来运行任务，同样的，我们可以指定特定网络运行：\nyarn hardhat block-number --network rinkeby Hardhat Console # 最后，除了通过代码与链/合约进行交互外，我们还可以通过 Hardhat Console 来调试项目，查看链状态，合约的输入、输出等。我们可以通过 yarn hardhat console 命令来打开 Hardhat Console，并进行交互。\n总结 # 以上就是我对 Hardhat 框架的基础配置与使用，它是一个很强大的开发框架，我后续还将会继续深入了解它的更多特性与使用技巧，如果有兴趣，可以继续关注，希望对大家有所帮助。\n参考资料 # Learn Blockchain, Solidity, and Full Stack Web3 Development with JavaScript NomicFoundation/hardhat Hardhat 官方文档 Solidity 智能合约开发 - 基础 Solidity 智能合约开发 - 玩转 Web3.py Solidity 智能合约开发 - 玩转 ethers.js "},{"id":9,"href":"/zh/docs/solidity/learn_solidity_from_scratch_basic/","title":"Solidity 智能合约开发 - 基础","section":"Docs","content":" Solidity 智能合约开发 - 基础 # 前言 # 去年读研的时候上的 HKU 的 \u0026lt;COMP7408 Distributed Ledger and Blockchain Technology\u0026gt;，课程中学习了以太坊智能合约的开发，做了一个简单的图书管理 ÐApp，然后毕业设计也选择了基于 Ethereum 做了一个音乐版权应用，详见 Uright - 区块链音乐版权管理ÐApp，对 Solidity 开发有一些基础了解。\n后来工作后主要做联盟链和业务开发这一块，很久没有碰过合约，对于语法和底层一些概念都已经一知半解，正好最近做的项目是基于 EVM 的一条链，涉及了一些基本的存证、回检和迁移相关合约的开发，调试起来有些吃力，于是打算系统学习一下，梳理一下笔记成文章，敦促自己好好思考总结。\n这系列文章也会收录在我的个人知识库项目 《区块链入门指南》中，希望在学习过程中不断完善。有兴趣的朋友也可以访问项目仓库参与贡献或提出建议。\n本文为系列第一篇，主要涉及 Solidity 基础知识。\n智能合约 与 Solidity 语言 # 智能合约是运行在链上的程序，合约开发者可以通过智能合约实现与链上资产/数据进行交互，用户可以通过自己的链上账户来调用合约，访问资产与数据。因为区块链保留区块历史记录的链式结构、去中心化、不可篡改等特征，智能合约相比传统应用来说能更公正、透明。\n然而，因为智能合约需要与链进行交互，部署、数据写入等操作都会消耗一定费用，数据存储与变更成本也比较高，因此在设计合约时需要着重考虑资源的消耗。此外，常规智能合约一经部署就无法进行修改，因此，合约设计时也需要多考虑其安全性、可升级性与拓展性。\nSolidity 是一门面向合约的、为实现智能合约而创建的高级编程语言，在 EVM 虚拟机上运行，语法整体类似于 Javascript，是目前最流行的智能合约语言，也是入门区块链与 Web3 所必须掌握的语言。针对上述的一些合约编写的问题，Solidity 也都有相对完善的解决方案支持，后续会详细讲解。\n开发/调试工具 # 与常规编程语言不同，Solidity 智能合约的开发往往无法直接通过一个 IDE 或本地环境进行方便的调试，而是需要与一个链上节点进行交互。开发调试往往也不会直接与主网（即真实资产、数据与业务所在的链）进行交互，否则需要承担高额手续费。目前开发调试主要有以下几种方式与框架：\nRemix IDE。通过 Ethereum 官方提供的基于浏览器的 Remix 开发工具进行调试，Remix 会提供完整的 IDE、编译工具、部署调试的测试节点环境、账户等，可以很方便地进行测试，这是我学习使用时用的最多的工具。Remix 还可以通过 MetaMask 插件与测试网、主网进行直接交互，部分生产环境也会使用它进行编译部署。 Truffle。Truffle 是一个非常流行的 Javascript 的 Solidity 合约开发框架，提供了完整的开发、测试、调试工具链，可以与本地或远程网络进行交互。 Brownie。Brownie 是一个基于 Python 的 Solidity 合约开发框架，以简洁的 Python 语法为调试和测试提供了便捷的工具链。 Hardhat。Hardhat 是另一个基于 Javascript 的开发框架，提供了非常丰富的插件系统，适合开发复杂的合约项目。 除了开发框架外，更好地进行 Solidity 还需要熟悉一些工具：\nRemix IDE 对于语法提示等并不完善，因此，可以使用 Visual Studio Code 配合 Solidity 进行编写，有更好的体验。 MetaMask。一个常用的钱包应用，开发过程中可以通过浏览器插件与测试网、主网进行交互，方便开发者进行调试。 Ganache。Ganache 是一个开源的虚拟本地节点，提供了一个虚拟链网络，可以通过各类 Web3.js、Remix 或一些框架工具与之交互，适合有一定规模的项目进行本地调试与测试。 Infura。Infura 是一个 IaaS（Infrastructure as a Service）产品，我们可以申请自己的 Ethereum 节点，通过 Infura 提供的 API 进行交互，可以很方便地进行调试，也更接近生产环境。 OpenZeppelin。OpenZeppelin 提供了非常多的合约开发库与应用，能兼顾安全、稳定的同时给予开发者更好的开发体验，降低合约开发成本。 合约编译/部署 # Solidity 合约是以 .sol 为后缀的文件，无法直接执行，需要编译为 EVM（Ethereum Virtual Machine）可识别的字节码才能在链上运行。\n编译完成后，由合约账户进行部署到链上，其他账户可通过钱包与合约进行交互，实现链上业务逻辑。\n核心语法 # 经过上文，我们对 Solidity 的开发、调试与部署有了一定了解。接下来我们就具体学习一下 Solidity 的核心语法。\n数据类型 # 与我们常见的编程语言类似，Solidity 有一些内置数据类型。\n基本数据类型 # boolean，布尔类型有 true 和 false 两种类型，可以通过 bool public boo = true; 来定义，默认值为 false int，整数类型，可以指定 int8 到 int256，默认为 int256，通过 int public int = 0; 来定义，默认值为 0，还可以通过 type(int).min 和 type(int).max 来查看类型最小和最大值 uint，非负整数类型，可以指定 uint8、uint16、uint256，默认为 uint256，通过 uint8 public u8 = 1; 来定义，默认值为 0 address，地址类型，可以通过 address public addr = 0xCA35b7d915458EF540aDe6068dFe2F44E8fa733c; 来定义，默认值为 0x0000000000000000000000000000000000000000 bytes，byte[] 的缩写，分为固定大小数组和可变数组，通过 bytes1 a = 0xb5; 来定义 还有一些相对复杂的数据类型，我们单独进行讲解。\nEnum # Enum 是枚举类型，可以通过以下语法来定义\nenum Status { Unknown, Start, End, Pause } 并通过以下语法来进行更新与初始化\n// 实例化枚举类型 Status public status; // 更新枚举值 function pause() public { status = Status.Pause; } // 初始化枚举值 function reset() public { delete status; } 数组 # 数组是一种存储同类元素的有序集合，通过 uint[] public arr; 来进行定义，在定义时可以预先指定数组大小，如 uint[10] public myFixedSizeArr;。\n需要注意的是，我们可以在内存中创建数组（关于 memory 与 storage 等差异后续会详细讲解），但是必须固定大小，如 uint[] memory a = new uint[](5);。\n数组类型有一些基本操作方法，如下：\n// 定义数组类型 uint[7] public arr; // 添加数据 arr.push(7); // 删除最后一个数据 arr.pop(); // 删除某个索引值数据 delete arr[1]; // 获取数组长度 uint len = arr.length; mapping # mapping 是一种映射类型，使用 mapping(keyType =\u0026gt; valueType) 来定义，其中键需要是内置类型，如 bytes、string、string 或合约类型，而值可以是任何类型，如嵌套 mapping 类型。需要注意的是，mapping 类型是不能被迭代遍历的，需要遍历则需要自行实现对应索引。\n下面说明一下各类操作：\n// 定义嵌套 mapping 类型 mapping(string =\u0026gt; mapping(string =\u0026gt; string)) nestedMap; // 设置值 nestedMap[id][key] = \u0026#34;0707\u0026#34;; // 读取值 string value = nestedMap[id][key]; // 删除值 delete nestedMap[id][key]; Struct # struct 是结构类型，对于复杂业务，我们经常需要定义自己的结构，将关联的数据组合起来，可以在合约内进行定义\ncontract Struct { struct Data { string id; string hash; } Data public data; // 添加数据 function create(string calldata _id) public { data = Data{id: _id, hash: \u0026#34;111222\u0026#34;}; } // 更新数据 function update(string _id) public { // 查询数据 string id = data.id; // 更新 data.hash = \u0026#34;222333\u0026#34; } } 也可以单独文件定义所有需要的结构类型，由合约按需导入\n// \u0026#39;StructDeclaration.sol\u0026#39; struct Data { string id; string hash; } // \u0026#39;Struct.sol\u0026#39; import \u0026#34;./StructDeclaration.sol\u0026#34; contract Struct { Data public data; } 变量/常量/Immutable # 变量是 Solidity 中可改变值的一种数据结构，分为以下三种：\nlocal 变量 state 变量 global 变量 其中， local 变量定义在方法中，而不会存储在链上，如 string var = \u0026quot;Hello\u0026quot;;；而 state 变量在方法之外定义，会存储在链上，通过 string public var; 定义变量，写入值时会发送交易，而读取值则不会；global 变量则是提供了链信息的全局变量，如当前区块时间戳变量，uint timestamp = block.timestamp;，合约调用者地址变量，address sender = msg.sender; 等。\n变量可以通过不同关键字进行声明，表示不同的存储位置。\nstorage，会存储在链上 memory，在内存中，只有方法被调用的时候才存在 calldata，作为调用方法传入参数时存在 而常量是一种不可以改变值的变量，使用常量可以节约 gas 费用，我们可以通过 string public constant MY_CONSTANT = \u0026quot;0707\u0026quot;; 来进行定义。immutable 则是一种特殊的类型，它的值可以在 constructor 中初始化，但不可以再次改变。灵活使用这几种类型可以有效节省 gas 费并保障数据安全。\n函数 # 在 Solidity 中，函数用来定义一些特定业务逻辑。\n权限声明 # 函数分为不同的可见性，用户不同的关键字进行声明：\npublic，任何合约都可调用 private，只有定义了该方法的合约内部可调用 internal，只有在继承合约可调用 external，只有其他合约和账户可调用 查询数据的合约函数也有不同的声明方式：\nview 可以读取变量，但不能更改 pure 不可以读也不可以修改 函数修饰符 # modifier 函数修饰符可以在函数运行前/后被调用，主要用来进行权限控制、对输入参数进行校验以及防止重入攻击等。这三种功能修饰符可以通过以下语法定义：\nmodifier onlyOwner() { require(msg.sender == owner, \u0026#34;Not owner\u0026#34;); _; } modifier validAddress(address _addr) { require(_addr != address(0), \u0026#34;Not valid address\u0026#34;); _; } modifier noReentrancy() { require(!locked, \u0026#34;No reentrancy\u0026#34;); locked = true; _; locked = false; } 使用函数修饰符则是需要在函数声明时添加对应修饰符，如：\nfunction changeOwner(address _newOwner) public onlyOwner validAddress(_newOwner) { owner = _newOwner; } function decrement(uint i) public noReentrancy { x -= i; if (i \u0026gt; 1) { decrement(i - 1); } } 函数选择器 # 当函数被调用时，calldata 的前四个字节要指定以确认调用哪个函数，被称为函数选择器。\naddr.call(abi.encodeWithSignature(\u0026#34;transfer(address,uint256)\u0026#34;, 0xSomeAddress, 123)) 上述代码 abi.encodeWithSignature() 返回值的前四个字节就是函数选择器。我们如果在执行前预先计算函数选择器的话可以节约一些 gas 费。\ncontract FunctionSelector { function getSelector(string calldata _func) external pure returns (bytes4) { return bytes4(keccak256(bytes(_func))); } } 条件/循环结构 # 条件 # Solidity 使用 if、else if、else 关键字来实现条件逻辑：\nif (x \u0026lt; 10) { return 0; } else if (x \u0026lt; 20) { return 1; } else { return 2; } 也可以使用简写形式：\nx \u0026lt; 20 ? 1 : 2; 循环 # Solidity 使用 for、while、do while 关键字来实现循环逻辑，但是因为后两者容易达到 gas limit 边界值，所以基本上不用。\nfor (uint i = 0; i \u0026lt; 10; i++) { // 业务逻辑 } uint j; while (j \u0026lt; 10) { j++; } 合约 # 构造器 # Solidity 的 constructor 可以在创建合约的时候执行，主要用来初始化\nconstructor(string memory _name) { name = _name; } 如果合约之间存在继承关系，constructor 也会按照继承顺序。\n接口 # Interface，通过声明接口来进行合约交互，有以下要求：\n不能实现任何方法 可以继承其他接口 的所有方法都必须声明为 external 不能声明构造方法 不能声明状态变量 接口用如下语法进行定义：\ncontract Counter { uint public count; function increment() external { count += 1; } } interface ICounter { function count() external view returns (uint); function increment() external; } 调用则是通过\ncontract MyContract { function incrementCounter(address _counter) external { ICounter(_counter).increment(); } function getCount(address _counter) external view returns (uint) { return ICounter(_counter).count(); } } 继承 # Solidity 合约支持继承，且可以同时继承多个，使用 is 关键字。\n函数可以进行重写，需要被继承的合约方法需要声明为 virtual，重写方法需要使用 override 关键字。\n// 定义父合约 A contract A { function foo() public pure virtual returns (string memory) { return \u0026#34;A\u0026#34;; } } // B 合约继承 A 合约并重写函数 contract B is A { function foo() public pure virtual override returns (string memory) { return \u0026#34;B\u0026#34;; } } // D 合约继承 B、C 合约并重写函数 contract D is B, C { function foo() public pure override(B, C) returns (string memory) { return super.foo(); } } 有几点需要注意的是，继承顺序会影响业务逻辑，state 状态变量是不可以被继承的。\n如果子合约想调用父合约，除了直接调用外，还可以通过 super 关键字来调用，如下：\ncontract B is A { function foo() public virtual override { // 直接调用 A.foo(); } function bar() public virtual override { // 通过 super 关键字调用 super.bar(); } } 合约创建 # Solidity 中可以从另一个合约中使用 new 关键字来创建另一个合约\nfunction create(address _owner, string memory _model) public { Car car = new Car(_owner, _model); cars.push(car); } 而 solidity 0.8.0 后支持 create2 特性创建合约\nfunction create2(address _owner, string memory _model, bytes32 _salt) public { Car car = (new Car){salt: _salt}(_owner, _model); cars.push(car); } 导入合约/外部库 # 复杂业务中，我们往往需要多个合约之间进行配合，这时候可以使用 import 关键字来导入合约，分为本地导入 import \u0026quot;./Foo.sol\u0026quot;; 与外部导入 import \u0026quot;https://github.com/owner/repo/blob/branch/path/to/Contract.sol\u0026quot;; 两种方式。\n外部库和合约类似，但不能声明状态变量，也不能发送资产。如果库的所有方法都是 internal 的话会被嵌入合约，如果非 internal，需要提前部署库并且链接起来。\nlibrary SafeMath { function add(uint x, uint y) internal pure returns (uint) { uint z = x + y; require(z \u0026gt;= x, \u0026#34;uint overflow\u0026#34;); return z; } } contract TestSafeMath { using SafeMath for uint; } 事件 # 事件机制是合约中非常重要的一个设计。事件允许将信息记录到区块链上，DApp 等应用可以通过监听事件数据来实现业务逻辑，存储成本很低。以下是一个简单的日志抛出机制：\n// 定义事件 event Log(address indexed sender, string message); event AnotherLog(); // 抛出事件 emit Log(msg.sender, \u0026#34;Hello World!\u0026#34;); emit Log(msg.sender, \u0026#34;Hello EVM!\u0026#34;); emit AnotherLog(); 定义事件时可以传入 indexed 属性，但最多三个，加了后可以对这个属性的参数进行过滤，var event = myContract.transfer({value: [\u0026quot;99\u0026quot;,\u0026quot;100\u0026quot;,\u0026quot;101\u0026quot;]});。\n错误处理 # 链上错误处理也是合约编写的重要环节。Solidity 可以通过以下几种方式抛出错误。\nrequire 都是在执行前验证条件，不满足则抛出异常。\nfunction testRequire(uint _i) public pure { require(_i \u0026gt; 10, \u0026#34;Input must be greater than 10\u0026#34;); } revert 用来标记错误与进行回滚。\nfunction testRevert(uint _i) public pure { if (_i \u0026lt;= 10) { revert(\u0026#34;Input must be greater than 10\u0026#34;); } } assert 要求一定要满足条件。\nfunction testAssert() public view { assert(num == 0); } 注意，在 Solidity 中，当出现错误时会回滚交易中发生的所有状态改变，包括所有的资产，账户，合约等。\ntry / catch 也可以捕捉错误，但只能捕捉来自外部函数调用和合约创建的错误。\nevent Log(string message); event LogBytes(bytes data); function tryCatchNewContract(address _owner) public { try new Foo(_owner) returns (Foo foo) { emit Log(\u0026#34;Foo created\u0026#34;); } catch Error(string memory reason) { emit Log(reason); } catch (bytes memory reason) { emit LogBytes(reason); } } payable 关键字 # 我们可以通过声明 payable 关键字设置方法可从合约中接收 ether。\n// 地址类型可以声明 payable address payable public owner; constructor() payable { owner = payable(msg.sender); } // 方法声明 payable 来接收 Ether function deposit() public payable {} 与 Ether 交互 # 与 Ether 交互是智能合约的重要应用场景，主要分为发送和接收两部分，分别有不同的方法实现。\n发送 # 主要通过 transfer、send 与 call 方法实现，其中 call 优化了对重入攻击的防范，在实际应用场景中建议使用（但一般不用来调用其他函数）。\ncontract SendEther { function sendViaCall(address payable _to) public payable { (bool sent, bytes memory data) = _to.call{value: msg.value}(\u0026#34;\u0026#34;); require(sent, \u0026#34;Failed to send Ether\u0026#34;); } } 而如果需要调用另一个函数，则一般使用 delegatecall。\ncontract B { uint public num; address public sender; uint public value; function setVars(uint _num) public payable { num = _num; sender = msg.sender; value = msg.value; } } contract A { uint public num; address public sender; uint public value; function setVars(address _contract, uint _num) public payable { (bool success, bytes memory data) = _contract.delegatecall( abi.encodeWithSignature(\u0026#34;setVars(uint256)\u0026#34;, _num) ); } } 接收 # 接收 Ether 主要用 receive() external payable 与 fallback() external payable 两种。\n当一个不接受任何参数也不返回任何参数的函数、当 Ether 被发送至某个合约但 receive() 方法未实现或 msg.data 非空时，会调用 fallback() 方法。\ncontract ReceiveEther { // 当 msg.data 为空时 receive() external payable {} // 当 msg.data 非空时 fallback() external payable {} function getBalance() public view returns (uint) { return address(this).balance; } } Gas 费 # 在 EVM 中执行交易需要耗费 gas 费，gas spent 表示需要多少 gas 量，gas price 为 gas 的单位价格，Ether 和 Wei 是价格单位，1 ether == 1e18 wei。\n合约会对 Gas 进行限制，gas limit 由发起交易的用户设置，最多花多少 gas，block gas limit，由区块链网络决定，这个区块中最多允许多少 gas。\n我们在合约开发中要尤其考虑尽量节约 gas 费，有以下几个常用技巧：\n使用 calldata 来替换 memory 将状态变量载入内存 使用 i++ 而不是 ++i 缓存数组元素 function sumIfEvenAndLessThan99(uint[] calldata nums) external { uint _total = total; uint len = nums.length; for (uint i = 0; i \u0026lt; len; ++i) { uint num = nums[i]; if (num % 2 == 0 \u0026amp;\u0026amp; num \u0026lt; 99) { _total += num; } } total = _total; } 总结 # 以上就是我们系列第一篇，Solidity 基础知识，后续文章会对其常见应用和实用编码技巧进行学习总结，欢迎大家持续关注。\n参考资料 # Solidity by Example Ethereum 區塊鏈！智能合約(Smart Contract)與分散式網頁應用(dApp)入門 区块链入门指南 Uright - 区块链音乐版权管理ÐApp "},{"id":10,"href":"/zh/docs/solidity/learn_solidity_from_scratch_web3py/","title":"Solidity 智能合约开发 - 玩转 Web3.py","section":"Docs","content":" Solidity 智能合约开发 - 玩转 Web3.py # 前言 # 在前文《Solidity 智能合约开发 - 基础》中，我们学习了 Solidity 的基本语法，并且了解了可以通过 Brownie 与 HardHat 等框架进行调试。但在使用这些封装好的框架之前，我们可以通过 Web3.py 直接与我们本地的 Ganache 节点进行交互，以便更好了解其原理，也为我们后续更好使用框架打好基础。\n本文以 Web3.py 为例，实现了基础的合约编译、部署至本地 Ganache 网络、与合约交互等功能。\n可以点击这里访问本测试 Demo 代码仓库。\nWeb3.py # Web3.py 是 Python 的一个开源库，它提供了一个简单的 API，可以让我们通过 Python 程序与以太坊网络进行交互。其 GitHub 地址为 ethereum/web3.py，可以访问其官方文档进行使用。\n安装 # 我们可以通过 Python 包管理工具 pip 安装 Web3.py，如下：\npip3 install web3 使用 # 使用 import 导入所需方法即可使用\nfrom web3 import Web3 w3 = Web3(Web3.HTTPProvider(\u0026#34;HTTP://127.0.0.1:7545\u0026#34;)) Solidity 合约编译 # 合约源码 # // SPDX-License-Identifier: MIT pragma solidity ^0.6.0; contract SimpleStorage { uint256 favoriteNumber; bool favoriteBool; struct People { uint256 favoriteNumber; string name; } People public person = People({favoriteNumber: 2, name: \u0026#34;Arthur\u0026#34;}); People[] public people; mapping(string =\u0026gt; uint256) public nameToFavoriteNumber; function store(uint256 _favoriteNumber) public returns (uint256) { favoriteNumber = _favoriteNumber; return favoriteNumber; } function retrieve() public view returns (uint256) { return favoriteNumber; } function addPerson(string memory _name, uint256 _favoriteNumber) public { people.push(People({favoriteNumber: _favoriteNumber, name: _name})); nameToFavoriteNumber[_name] = _favoriteNumber; } } 这是一个简单的存储合约，通过一个 People 结构体对象来存储人名和他喜欢数字，通过一个数组来存储多个人的信息，并提供了添加、查找方法。\n读取合约源文件 # 当我们通过 VSCode 或其他编辑器完成 Solidity 合约编写与语法检查后，需要读取合约源文件并存入变量，供后续编译使用。\nimport os with open(\u0026#34;./SimpleStorage.sol\u0026#34;, \u0026#34;r\u0026#34;) as file: simple_storage_file = file.read() 上述代码将 SimpleStorage.sol 文件内容读取到变量 simple_storage_file 中。\n编译合约 # 安装 solcx # 合约编译需要预先安装 solcx 工具。\npip3 install py-solc-x 导入 solcx # 使用 import 导入所需方法即可使用\nfrom solcx import compile_standard, install_solc 编译 # install_solc(\u0026#34;0.6.0\u0026#34;) compiled_sol = compile_standard( { \u0026#34;language\u0026#34;: \u0026#34;Solidity\u0026#34;, \u0026#34;sources\u0026#34;: {\u0026#34;SimpleStorage.sol\u0026#34;: {\u0026#34;content\u0026#34;: simple_storage_file}}, \u0026#34;settings\u0026#34;: { \u0026#34;outputSelection\u0026#34;: { \u0026#34;*\u0026#34;: {\u0026#34;*\u0026#34;: [\u0026#34;abi\u0026#34;, \u0026#34;metadata\u0026#34;, \u0026#34;evm.bytecode\u0026#34;, \u0026#34;evm.sourceMap\u0026#34;]} } }, }, solc_version=\u0026#34;0.6.0\u0026#34;, ) 上述代码我们安装了 0.6.0 版本的 Solidity 编译程序，使用 solcx 库中的compile_standard 方法对上文读取的合约源文件进行编译，并将编译结果存入变量 compiled_sol 中。\n获取编译结果 # 编译成功后，使用以下代码将编译好的合约写入文件\nimport json with open(\u0026#34;compiled_code.json\u0026#34;, \u0026#34;w\u0026#34;) as file: json.dump(compiled_sol, file) 获取 bytecode 与 abi # Solidity 合约的部署与交互需要 bytecode 与 abi 两个部分，我们可以通过通过以下代码将其写入对应变量供后续操作使用。\n# get bytecode bytecode = compiled_sol[\u0026#34;contracts\u0026#34;][\u0026#34;SimpleStorage.sol\u0026#34;][\u0026#34;SimpleStorage\u0026#34;][\u0026#34;evm\u0026#34;][ \u0026#34;bytecode\u0026#34; ][\u0026#34;object\u0026#34;] # get abi abi = compiled_sol[\u0026#34;contracts\u0026#34;][\u0026#34;SimpleStorage.sol\u0026#34;][\u0026#34;SimpleStorage\u0026#34;][\u0026#34;abi\u0026#34;] 本地 Ganache 环境 # 智能合约的调试需要将合约部署到实际的链上，而部署到 Ethereum 主网络或 Rinkeby/Koven 等测试网等也不方便调试，因此，我们需要一个本地的区块链环境，Ganache 就给我们提供了一个这样的本地调试环境。Ganache 主要分为 GUI 和 CLI 两种安装方式。\nGanache GUI # 在自己的本地环境，如 Mac/Windows 等系统，我们可以选择带图形界面的 Ganache 客户端，安装与使用都十分便捷，在 Ganache 官网选择对应版本即可。\n安装完成后选择 Quick Start 即可快速启动一条本地运行的区块链网络，并初始化了十个拥有 100 ETH 的账户，开发调试过程中可使用。\nGanache CLI 安装 # 如果您的系统不支持 GUI 安装，我们可以使用 CLI 安装，安装方式如下：\nnpm install --global yarn yarn global add ganache-cli 等待其安装完成后即可启动本地测试网络，与 Ganache GUI 一致，也包含初始化账户与余额。\n通过 web3 连接本地 Ganache 环境 # web3 提供了库可以方便地连接到本地 Ganache 环境：\nw3 = Web3(Web3.HTTPProvider(\u0026#34;HTTP://127.0.0.1:7545\u0026#34;)) chain_id = 5777 my_address = \u0026#34;0x2F490e1eA91DF6d3cC856e7AC391a20b1eceD6A5\u0026#34; private_key = \u0026#34;0fa88bf96b526a955a6126ae4cca0e72c9c82144ae9af37b497eb6afbe8a9711\u0026#34; Solidity 合约部署 # 创建合约 # 我们可以通过 web3 库创建合约。\nSimpleStorage = w3.eth.contract(abi=abi, bytecode=bytecode) 部署合约 # 部署合约分为三个主要步骤：\n构造交易 签名交易 发送交易 构造交易 # nonce = w3.eth.getTransactionCount(my_address) transaction = SimpleStorage.constructor().buildTransaction( { \u0026#34;chainId\u0026#34;: chain_id, \u0026#34;gasPrice\u0026#34;: w3.eth.gas_price, \u0026#34;from\u0026#34;: my_address, \u0026#34;nonce\u0026#34;: nonce, } ) 签名交易 # signed_txn = w3.eth.account.sign_transaction(transaction, private_key=private_key) 发送交易 # tx_hash = w3.eth.send_raw_transaction(signed_txn.rawTransaction) tx_receipt = w3.eth.wait_for_transaction_receipt(tx_hash) 与合约交互 # 与部署合约步骤类似，我们可以通过 web3 库与合约交互，也分为构造交易、签名交易和发送交易三个步骤。\n构造交易 # simple_storage = w3.eth.contract(address=tx_receipt.contractAddress, abi=abi) store_transaction = simple_storage.functions.store(67).buildTransaction( { \u0026#34;chainId\u0026#34;: chain_id, \u0026#34;gasPrice\u0026#34;: w3.eth.gas_price, \u0026#34;from\u0026#34;: my_address, \u0026#34;nonce\u0026#34;: nonce + 1, } ) 签名交易 # signed_store_txn = w3.eth.account.sign_transaction( store_transaction, private_key=private_key ) 发送交易 # send_store_tx = w3.eth.send_raw_transaction(signed_store_txn.rawTransaction) tx_receipt = w3.eth.wait_for_transaction_receipt(send_store_tx) 总结 # 以上就是我们通过 Web3.py 库与本地 Ganache 测试网络进行交互的步骤，在真正的生产项目开发中我们一般不会直接使用 Web3.py 这样的库，而是会使用 Brownie、HardHat 等进一步封装的库，但了解 Web3.py 或 Web3.js 等库的使用方法也非常重要。\n参考资料 # Solidity 智能合约开发 - 基础 ethereum/web3.py Solidity, Blockchain, and Smart Contract - Beginner to Expert Full Course | Python Edition "},{"id":11,"href":"/zh/docs/solidity/two_phase_commit_contract_practice_in_solidity/","title":"Solidity 智能合约开发 - 玩转 Web3.py","section":"Docs","content":" 通过状态锁在 Solidity 智能合约中实现两阶段提交 # 前言 # 在一些牵扯到多个系统或合约交互的智能合约应用场景中，尤其是一些资产/数据准确性较为敏感的业务中，我们需要保证在整个业务流程中数据的原子性。因此，我们需要在合约层面实现类似多阶段提交的机制，即将合约中的状态更改过程分解为预提交和正式提交两个阶段。\n本文通过状态锁的机制实现了一个最小化的两阶段提交模型，完整合约代码参见 TwoPhaseCommit.sol，下文将对本合约核心逻辑进行讲解，并尽量遵循风格指南与最佳实践。\n注：本合约因初始场景主要考虑的是联盟链中的业务用途，未对 Gas fee 等进行特定优化，仅供学习参考。\n合约逻辑 # 合约结构 # 两阶段提交场景包含以下方法：\nset: 两阶段 - 预提交 commit: 两阶段 - 正式提交 rollback: 两阶段 - 回滚 因 Solidity 语言对于字符串长度判断/比较等有一些限制，为了提升合约代码的可读性，本合约提供了部分辅助方法，主要包含以下方法：\nisValidKey: 检查 key 是否合法 isValidValue: 检查 value 是否合法 isEqualString: 比较两个字符串是否相等 两阶段提交核心逻辑 # 在两阶段提交场景中，本合约提供了一套简易的 set, commit, rollback 方法实现，实现了将合约调用传入的 key-value 键值对存储到链上。我们通过状态锁的机制来实现跨链交易的原子性。我们定义了如下数据结构：\nenum State { UNLOCKED, LOCKED } struct Payload { State state; string value; string lockValue; } 其中，State 为枚举类型，记录了链上 key 值的锁定状态，而 Payload 结构则会对锁定状态、当前值与正在锁定的值进行存储，并通过如下 mapping 结构与 key 进行绑定：\nmapping (string =\u0026gt; Payload) keyToPayload; 因此，我们可以根据 keyToPaylaod 对合约调用中的每一个 key 进行状态跟踪，并在下述 cc_set, cc_commit, cc_rollback 方法中对 key 的状态进行检查，进行一些异常处理。\ncc_set() # 在 cc_set() 方法中，我们会检查 key 的状态，如为 State.LOCKED，则不会进行存储并抛出异常：\nif (keyToPayload[_key].state == State.LOCKED) { revert TwoPhaseCommit__DataIsLocked(); } 如为 State.UNLOCKED，则会将合约调用传入的值存储至 lockValue 中，并将其状态设置为 LOCKED，等待后续 cc_commit 或 cc_rollback 进行解锁。\nkeyToPayload[_key].state = State.LOCKED; keyToPayload[_key].lockValue = _value; cc_commit() # 在 cc_commit() 方法中，我们会检查 key 的状态，如为 State.UNLOCKED，则不会对该 key 进行操作，并抛出异常：\nif (keyToPayload[_key].state == State.UNLOCKED) { revert TwoPhaseCommit__DataIsNotLocked(); } 如为 State.LOCKED，我们检查合约调用传入的值是否与 lockValue 相等，如不相等，则抛出异常：\nif (!isEqualString(keyToPayload[_key].lockValue, _value)) { revert TwoPhaseCommit__DataIsInconsistent(); } 如值相等，则会将该 key 所对应的 value 存储上链，将 key 的状态设置为 UNLOCKED，更新当前值 value，同时将 lockValue 置空：\nstore[_key] = _value; keyToPayload[_key].state = State.UNLOCKED; keyToPayload[_key].value = _value; keyToPayload[_key].lockValue = \u0026#34;\u0026#34;; cc_rollback() # 在 cc_rollback() 方法中，我们会检查 key 的状态，如为 State.UNLOCKED，则不会对该 key 进行操作，并抛出异常：\nif (keyToPayload[_key].state == State.UNLOCKED) { revert TwoPhaseCommit__DataIsNotLocked(); } 如为 State.LOCKED，我们检查合约调用传入的值是否与 lockValue 相等，如不相等，则抛出异常：\nif (!isEqualString(keyToPayload[_key].lockValue, _value)) { revert TwoPhaseCommit__DataIsInconsistent(); } 如值相等，则会将该 key 所对应的 value 存储上链，将 key 的状态设置为 UNLOCKED，并将 lockValue 置空：\nkeyToPayload[_key].state = State.UNLOCKED; keyToPayload[_key].lockValue = \u0026#34;\u0026#34;; 错误处理逻辑 # 在合约执行异常场景中，我们会抛出错误并进行回滚。为了更好地提升错误消息的可读性并方便上层应用人员进行错误捕获与处理，我们采用了错误类型定义的方式，定义了各类异常场景，因为我在错误命名中已经包含了大部分信息，所以未定义错误类型额外参数值，可以根据需求自行定制。\nerror TwoPhaseCommit__DataKeyIsNull(); error TwoPhaseCommit__DataValueIsNull(); error TwoPhaseCommit__DataIsNotExist(); error TwoPhaseCommit__DataIsLocked(); error TwoPhaseCommit__DataIsNotLocked(); error TwoPhaseCommit__DataIsInconsistent(); 在具体合约逻辑中，我们通过 revert 方法抛出异常，如：\nif (!isValidKey(bytes(_key))) { revert TwoPhaseCommit__DataKeyIsNull(); } if (!isValidValue(bytes(_value))) { revert TwoPhaseCommit__DataValueIsNull(); } if (keyToPayload[_key].state == State.UNLOCKED) { revert TwoPhaseCommit__DataIsNotLocked(); } if (!isEqualString(keyToPayload[_key].lockValue, _value)) { revert TwoPhaseCommit__DataIsInconsistent(); } 通用参数校验 # 我们会对传入参数进行一些合法性校验，为了提供拓展性，我们通过 isValidKey() 与 isValidValue() 方法对 key 与 value 进行独立校验：\n/** * @notice 数据键格式校验 * @param _key 数据 - 键 */ function isValidKey(bytes memory _key) private pure returns (bool) { bytes memory key = _key; if (key.length == 0) { return false; } return true; } /** * @notice 数据值格式校验 * @param _value 数据 - 值 */ function isValidValue(bytes memory _value) private pure returns (bool) { bytes memory value = _value; if (value.length == 0) { return false; } return true; } 本合约只进行了非空校验，可根据业务需要自行定制业务逻辑，在需要校验的地方调用即可，如：\nif (!isValidKey(bytes(_key))) { revert TwoPhaseCommit__DataKeyIsNull(); } if (!isValidValue(bytes(_value))) { revert TwoPhaseCommit__DataValueIsNull(); } if (!isValidValue(bytes(store[_key]))) { revert TwoPhaseCommit__DataIsNotExist(); } 事件机制 # 此外，我们定义了核心方法对应的 event，并为事件设置了 indexed 以方便上层应用进行监听和处理。\nevent setEvent(string indexed key, string indexed value); event getEvent(string indexed key, string indexed value); event commitEvent(string indexed key, string indexed value); event rollbackEvent(string indexed key, string indexed value); 在合约方法中通过 emit() 方法抛出 event，如：\nemit setEvent(_key, _value); emit getEvent(_key, _value); emit commitEvent(_key, _value); emit rollbackEvent(_key, _value); 总结 # 以上就是我两阶段提交合约的一个最佳实践，关于 Solidity 基础语法可参看『Solidity 智能合约开发 - 基础』，后续我还会对更多合约场景进行实践与讲解，敬请关注。\n参考资料 # TwoPhaseCommit.sol 合约源码 Solidity 智能合约开发 - 基础 Solidity 官方文档 "},{"id":12,"href":"/zh/docs/bitcoin/blockchain_bitcoin_basic/","title":"比特币核心技术解读","section":"Docs","content":" 比特币核心技术解读 # 前言 # 在上一篇文章《区块链基础知识与关键技术》里对区块链的基础知识和关键技术进行了梳理，而比特币是区块链最典型的应用，本文将对比特币核心技术进行解读，如有错漏，欢迎交流指正。\n比特币系统 # 比特币是在 2009 年由中本聪发明的一个数字货币，主要是为了反抗中心化的银行体系，因为其精巧的系统设计和安全性，价值也在迅速提升。同时，因为它并不与真实世界的身份绑定，具备强大的匿名性，也被用于非法交易、洗钱、勒索等恶意行为，引起了一些争议。\n作为一个去中心化的区块链系统，所有人都可以访问，也可以在本地维护一个节点参与到比特币网络中，下文也会应用Bitcoin Core客户端在本地维护一个节点。\n节点分为全节点和轻节点两种，早期所有的节点都是全节点，但随着数据量越来越大，运行在手机或平板等设备上的比特币客户端不需要存储整个区块链的信息，称为Simplified Payment Verification(SPV)节点，也叫轻节点。\nBitcoin Core客户端就是一个全节点，下文也会具体讲述。全节点一直在线，维护着完整的区块链信息；因为其内存里维护着完整的UTXO集合，所以通过验证整个区块链的区块和交易信息（从创世区块到最新区块）来验证交易的合法性；也会决定哪些交易会被打包到区块中；验证交易即挖矿，可以决定沿着哪条链继续挖，在出现等长的分叉时，也会选择哪一个分叉；同时监听别的矿工挖出来的区块，验证合法性。\n轻节点不需要一直在线，也不需要保留整个区块链（数据量庞大），只需要保留每个区块的块头；且只需要保存与自己有关的区块，而不需要保存链上全部交易；因为并没有保存全部信息，无法验证大多数交易的合法性和网上发布的新区块的正确性，只能检验与自己有关的区块；可以通过Merkle Proof验证一笔交易存在，但不能确认一笔交易不存在；可以验证挖矿的难度，因为保存在块头中。\n下面通过一个示例来讲解一下全节点和轻节点的交易验证方式。\n假如要验证一个位于 block 300,000 的交易 T，全节点会查验全部 300,000 个区块（直到创世区块），建立一个完整UTXO的数据库来确保这个交易没有被花费；而轻节点则会通过Merkle Path来链接所有和交易 T 相关的区块，然后等待 300,001 至 300,006 个区块来进行确认，从而验证交易的合法性。\n区块链结构 # 区块链是由顺序链接起来的区块组成的一种数据结构，可以存于单文件或者数据库中，Bitcoin Client使用 Google 的LevelDB数据库存储数据。每一个区块都指向前一个区块，任何一个区块进行了修改的话，其所有后面的区块都会受到影响，所以想要篡改一个区块的话需要同时篡改之后的所有区块，这需要大量的算力，往往成本大于收益，因此极大地保障了安全性。\n区块链结构包含区块Block Size (4 bytes)、Block Header、Transaction Counter(1-9 bytes)和Transaction几个核心组成部分。\n区块链的块头大小为 80 bytes，存储着Version(4 bytes)、Previous Block Hash(32 bytes)、Merkle Tree Root(32 bytes)、Timestamp(4 bytes)、Difficulty Target(4 bytes)和Nonce(4 bytes)。\n每一个区块的哈希值通过对区块头进行两次哈希运算，即SHA256(SHA256(Block Header))，并不存在区块链结构中，而是由每个节点接收到区块后计算得到，是独一无二的；此外，Block Height也可以作为区块的标识符。\nMerkle Tree # Merkle Tree默克尔树是区块链中很重要的一个数据结构，主要通过哈希算法来验证较大数据集（也是通过双重哈希的方式SHA256(SHA256(Block Header))），结构如下图所示：\n通过Merkle Tree的方式可以很快地验证一个交易存在于某个区块中（算法复杂度为LogN），例如，如果要验证一个交易 K 存在于区块中，只需要访问很少的节点\n因为比特币网络中存在大量交易，这种方式能够极大提高效率，如下图所示：\n因为轻节点（例如手机上的比特币钱包）不保存整个区块链数据，通过Merkle Tree结构可以很方便地查找交易，轻节点会构造一个Bloom filter布隆过滤器来得到与自身相关的交易：\n首先，初始化布隆过滤器为空值，获取钱包中的所有地址，创建一个检索模式来匹配与这个交易输出相关的地址，将检索模式加入布隆过滤器； 然后布隆过滤器被发送至各个节点（通过filterload消息）； 节点收到后会发送一个包含符合条件的区块头和符合交易的Merkle Path的merkleblock消息和一个包含过滤结果的tx消息。 过程中，轻节点会使用Merkle Path来链接交易与区块，并通过区块头来组成区块链，从而能够验证交易存在于区块链中。\n使用布隆过滤器会返回符合筛选条件的结果，也会存在着一些误报，因此返回了很多不相关的结果，也能够在轻节点向其他节点请求相关地址时保护了隐私性。\n比特币网络 # 比特币系统运行在一个 P2P 点对点网络上，节点之间是平等的，没有身份、权限的区别；没有中心化的服务器，网络也没有层级区分。\n每个节点都要维护一个等待上链的交易的集合，每个区块大小为 1M，因此需要几秒才能够穿到大多数的节点。假设一个节点监听到了 A-\u0026gt;B 的交易，会将其写入集合，如果同时又发现了一个 A-\u0026gt;C 的双花攻击，则不会再写入，而如果监听到同样一笔 A-\u0026gt;B 的交易或者同一个币来源的 A-\u0026gt;C 的交易，则会将该集合中 A-\u0026gt;B 的交易删除。\n比特币共识协议 # 比特币作为一个人人都可以参与的开发系统，需要解决恶意节点的威胁，解决思路为工作量证明机制，也就是算力投票机制，当产生一笔新交易，广播新的数据记录，全网执行共识算法，即矿工挖矿来验证记录，即求解随机数，率先解出难题的矿工获得记账权，产生新区块，然后对外广播新区块，其他节点验证通过后加至主链。\n钱包 # 作为一个数字货币系统，比特币有自己的钱包系统，主要由私钥、公钥和钱包地址三个部分组成。\n生成钱包地址的过程如下：\n采用ECDSA(Elliptic Curve Digital Signature Algorithm)椭圆曲线算法，利用私钥生成对应的公钥 公钥很长且难以输入和记忆，因此再通过SHA256和RIPEMD160算法得到一个公钥哈希值 最后再用Base58Check进行处理，得到一个可读性较强的钱包地址 交易过程 # 有了钱包（和资产）后，就可以开始交易了。我们来通过一个典型的比特币交易来理解这一流程：\nA 和 B 都拥有一个比特币钱包地址（可以用 Bitcoin Client 生成，原理如上），假设 A 要给 B 转账 5 个 BTC，A 需要得到 B 的钱包地址，然后用自己的私钥对A-\u0026gt;B转账5个BTC这笔交易签名（因为 A 的私钥仅有自己知道，所以拥有私钥则是拥有钱包资产的归属权）；然后发布这笔交易，在比特币系统中发起交易需要支付小额矿工费作为交易手续费；矿工会开始验证这笔交易的合法性，得到六个确认后交易就可以被比特币账本所接受，整个验证过程大约 10 分钟。\n矿工为什么要消耗大量算力来验证交易呢？\n矿工在验证过程中可以得到出块奖励和矿工费，出块奖励会四年递减，因此，后期主要激励是矿工费。\n为什么验证要 10 分钟呢？\n比特币其实并不是绝对安全的，新交易容易受到一些恶意攻击，而通过控制挖矿难度把验证过程控制在 10 分钟左右则可以很大程度上阻止恶意攻击，这只是一种概率上的保证。\n比特币系统中怎么避免双重花费呢？\n比特币采用了一种叫UTXO(Unspent Transaction Outputs)的概念，当一个用户收到一笔 BTC 交易时，会计入UTXO中。\n在这个示例中，A 想要给 B 转账 5 个 BTC，A 的这 5 个 BTC 可能来自于两个UTXO(2 BTC + 3 BTC)，因此 A 在转账给 B 时，矿工需要检验的是这两笔UTXO在这笔交易之前有没有被花掉，如果检测已经被花费了，则交易不合法。\n下图很好地阐释了多笔交易的流向和UTXO的相关概念\n此外，UTXO有一个很重要的特性，不可分割，假如 A 有 20 个 BTC，他想转账 5 个 BTC 给 B，那交易会先将 20 个 BTC 作为输入，然后产生两个输出，一个向 B 转账 5 个 BTC，一个返还给 A 剩下的 15 个 BTC，因此，A 又拥有了一笔价值为 15 BTC 的 UTXO；如果单个UTXO不够支付，则可以组合多个形成输入，但总额一定要大于交易额。\n矿工怎么验证交易发起者有足够的余额呢？\n这个问题看起来很简单，第一反应是像支付宝这样查询一下余额是否足够就可以。但比特币是一种基于交易的账本模式，并没有帐户概念，因此并不能直接查询余额，要想知道一个帐户的剩余资产，则需要回顾以前所有的交易，并且找到所有UTXO并相加。\n交易模型 # 上文讲了一个交易是怎么发生的，那比特币交易由哪些部分组成呢？\n如图，最开始的部分是Version，表示版本。\n然后是 Input 相关的信息：Input Count表示数量，Input Info表示输入的内容，也就是Unlocking Script，主要用于核对输入来源、输入是否可用以及其他输入的细节。\nPrevious output hash - 所有输入都能追溯回一个输出，这指向包含将在该输入中花费的 UTXO，该 UTXO 的哈希值在这里以相反的顺序保存 Previous output index - 一个交易可以有多个由它们的索引号引用的UTXO，第一个索引是 0 Unlocking Script Size - Unlocking Script的字节大小 Unlocking Script - 满足UTXO Unlocking Script的哈希 Sequence Number - 默认为ffffffff 接着是 Output 相关的信息，Output Count表示数量，Output Info表示输出的内容，也就是Locking Script,主要用于记录输出了多少比特币，未来支出的条件以及输出的细节。\nAmount - 以 Satoshis(最小的比特币单位)表示的输出比特币数量，10^8 Satoshis = 1 比特币 Locking Script Size - 这是 Locking Script 的字节大小 Locking Script - 这是 Locking Script 的哈希，它指定了使用此输出必须满足的条件 最后是Locktime，表示一个交易可以被最早添加到区块链的时间/块，如果小于 500 million 的话直接读取块高度，而如果大于 500 million 则读取时间戳。\n比特币脚本 # 在交易中有提到Unlocking script和Locking script，那什么是比特币脚本呢？\n比特币脚本是记录在每个交易中的指令列表，当脚本被执行时可以检验交易是否有效、比特币是否可以使用等。一个典型的脚本如下\n\u0026lt;sig\u0026gt; \u0026lt;pubKey\u0026gt; OP_DUP OP_HASH160 \u0026lt;pubKeyHash\u0026gt; OP_EQUALVERIFY OP_CHECKSIG 比特币脚本是基于栈从左至右执行的，使用Opcodes对数据进行操作，在上面这个脚本语言中，\u0026lt;\u0026gt;包含的是要被推入 stack 的数据，没有\u0026lt;\u0026gt;包括、以 OP_ 为前缀的是操作符（OP 可省略），脚本也可以嵌入数据永久记录在链上（不超过 40 bytes），所记录的数据不会影响UTXO。\n在交易中，\u0026lt;sig\u0026gt; \u0026lt;pubKey\u0026gt;是Unlocking script，OP_DUP OP_HASH160 \u0026lt;pubKeyHash\u0026gt; OP_EQUALVERIFY OP_CHECKSIG部分是Locking script。\n跟大多数编程语言相比，比特币脚本是非图灵完备的，没有循环或复杂的流程控制，执行起来很简单，不论在哪里执行结果都是确定性的，也不会保存状态，且脚本与脚本之间是相互独立的。因为以上特征，虽然比特币脚本相对安全，但没办法处理很复杂的逻辑，因此不适合用来处理一些复杂的业务，Ethereum所提供的智能合约就在这一点上实现了创新性的突破，因此诞生了很多去中心化应用。\n挖矿 # 在上文对整个交易过程中提到了挖矿，接下来我们详细讲一下。\n有的节点为了得到出块奖励和矿工费，赚取收益，因此会对交易进行验证，称为矿工挖矿。出块奖励由coinbase创建，每四年会递减，从 2009 年的 25 个，到现在已经减少为 6.5 个。\n挖矿其实是一个不断尝试随机数以达到某个设定目标值的过程，如小于某个 target 值，这个难度是人为设置来调整验证时间、提升安全性的，而不是解决数学难题。\n矿工们会不断尝试这个值，成功率很低，但是尝试次数可以很多，因此，算力强的节点有成比例的优势，更容易解出难题。\n那挖矿难度为什么要进行调整呢？\n因为在比特币系统中，出块时间太短容易出现分叉，如果分叉过多则会影响系统达成共识，危害系统安全性。比特币系统通过难度调整把出块速度稳定在 10 分钟左右，从而防止交易被算改。\n挖矿难度是如何调整的呢？\n系统会在每产生 2016 个区块时（约两周）调整一次目标阈值，存在块头中，全网所有节点需要遵从新的难度进行挖矿，如果恶意节点不调整代码中的 target 的话，诚实的矿工则不会认可\n目标阈值 = 目标阈值 * (产生 2016 个区块的实际时间 / 产生 2016 个区块的预计时间)\n比特币诞生之初，矿工很少，挖矿难度也较低，大多都是用家用电脑（CPU）直接挖矿；随着越来越多的人参与到比特币生态中，挖矿的难度也越来越高，慢慢开始用一些算力较强的 GPU 进行挖矿，也有一些专用的ASIC(Application Specific Integrated circuit)专用挖矿芯片以及矿机随着市场需求逐步诞生；而现在也出现了很多大型矿池，集合了全网大量算力进行集中挖矿。\n在这种大型矿池系统中，Pool Manager担任了全节点的作用，而集合的大量矿工会一起计算哈希值，最后通过工作量证明机制来分配收益。但算力过于集中容易产生一些中心化风险，如某个大型矿池达到了全网 51% 以上算力的话就可以对交易进行回滚或者对某些交易进行抵制等。\n分叉 # 比特币系统中，也会有未达成一致性意见的情况发生，称为分叉。分叉是主要分为两种类型，一种是状态分叉，往往是一些节点故意进行的；另一种称为协议分叉，也就是说对比特币协议产生了一些分歧。\n协议分叉又可以分为两种类型，一种叫硬分叉，也就是对于协议的部分内容产生了不可兼容的修改，比如将比特币的块大小由 1M 调整为 4M，这种分叉方式是永久的，从某个节点开始形成了两条平行发展的链，比如Bitcoin Classic，形成了两种币。\n另一种则叫软分叉，比如还是调整比特币的块大小，但是从 1M 调整为 0.5M，这样调整后，就会出现新节点挖小区块，旧的节点挖大的区块的情况，软分叉是非永久性的，比较典型的例子是对 coinbase 的内容进行修改以及P2SH(Pay to Script Hash)产生的分叉。\nBitcoin Core 客户端 # Bitcoin Core是比特币的实现，又被称为Bitcoin-QT或Satoshi-client，可以通过这个客户端连接至比特币网络、验证区块链、发送与接收比特币等。有Mainnet、Testnet和Regnet三个网络，可以进行切换。\n提供了一个Debug Console来与比特币区块链直接进行交互，常见操作如下：\nBlockchain\ngetblockchaininfo: 返回有关区块链处理的各种状态信息 getblockcount: 返回区块链中的块数 verifychain: 验证区块链数据库 Hash\ngetblockhash: 返回所提供的区块哈希值 getnetworkhashps: 基于指定数量的最近块，返回每秒网络哈希数 getbestblockhash: 返回最佳块的哈希值 Blocks\ngetblock: 返回块信息的详细信息 getblockheader: 返回有关区块头信息 generate: 立即将指定数量的块挖矿到钱包中的一个地址 Wallet\ngetwalletinfo: 返回一个对象，该对象包含有关钱包状态的各种信息 listwallets: 返回当前加载的钱包列表 walletpassphrasechange: 更改钱包密码 Mempool\ngetmempoolinfo: 返回内存池活动状态的详细信息 getrawmempool: 返回内存池中的所有交易详细信息 getmempoolentry: 返回给定交易的内存池数据 Transaction\ngetchaintxstats: 计算关于链中交易总数和速率的统计数据 getrawtransaction: 返回原始交易数据 listtransactions: 返回给定帐户的交易列表 Signature\nsignrawtransaction: 签署原始交易的输入 signmessage: 使用地址的私钥对信息进行签名 dumpprivkey: 获取私钥 Network\ngetnetworkinfo: 返回 P2P 网络的状态信息 getpeerinfo: 返回每个连接网络节点的数据 getconnectioncount: 返回节点的连接数 Mining\ngetmininginfo: 返回包含挖掘相关信息的对象 getblocktemplate: 返回构造块所需的数据 prioritisetransaction: 以较高或较低的优先级接受交易进入挖掘的块 总结 # 以上就是对比特币核心技术的一些解读，主要从它的基础原理和数据模型层面进行了一些深入了解，通过对比特币的学习，能够很好地理解区块链的设计理念和运行机制，接下来将会对被称为区块链 2.0 的以太坊进行学习和分析，敬请期待！\n参考资料 # COMP7408 Distributed Ledger and Blockchain Technology, Professor S.M. Yiu, HKU Udacity Blockchain Developer Nanodegree, Udacity 区块链技术与应用，肖臻，北京大学 区块链技术进阶与实战，蔡亮 李启雷 梁秀波，浙江大学 | 趣链科技 "},{"id":13,"href":"/zh/docs/crosschain/blockchain_crosschain/","title":"跨链技术原理与实战","section":"Docs","content":" 跨链技术原理与实战 # 前言 # 目前区块链底层平台日渐多样，如老牌的 Hyperledger Fabric、Ethereum 等，以及国内的 Hyperchain、Z-ledger 等，而随着区块链应用生态越来越复杂，单链的性能有一定瓶颈，链与链之间的协同与交互（信息同步、共享、合约互操作等）也成为了链和应用生态发展的重要部分。\n本文是对跨链技术的概念与主流解决方案的梳理。\n跨链技术概览 # 因为底层链设计、共识算法、网络结构等组件的相似性，同构区块链之间的交互比较容易，但异构区块链则相对复杂，往往难以直接进行交互，而需要两条链之间有一些辅助平台/服务来进行数据格式转换等。\n跨链机制 # 目前跨链主要由以下几种解决方案：\n公证人机制 哈希锁定 分布式私钥控制 侧链/中继链 公证人机制 # 公证人机制是一种通过第三方中介协助不同链之间交互的机制，本质上是两方共同信任一个第三方，让其对跨链数据或跨链交互操作进行验证和转发。这种方式能很好地支持异构区块链，但是是一种中心化方式。\n很多数字货币交易所就是通过这样的方式进行不同数字货币之间的交易和转换，本质上是交易所在撮合交易，效率等都较高，但是存在一定安全风险，且只支持资产的交换。\n哈希锁定 # 哈希锁定最早出现在比特币的闪电网络，是通过哈希锁和时间锁保障跨链双方资产的一种方式。其中时间锁是将交易限制在一定时间内，超时则交易失效，从而避免损失，但这种方式同样只能实现资产的交换，而无法实现资产的转移。\n侧链 # 侧链是一种双向锚定的技术，最开始的侧链是相对于比特币主链而言的，如 BTC-Relay，在这条侧链上可以对比特币进行新特性的研发和测试，且当大量用户在比特币网络上进行交易时，使用侧链可以有效地拓展网络的吞吐量。例如，在 Ethereum 主链上进行资产交易和价值转移，而在 Ethereum 侧链上可以进行一些对 tps 要求较高的 DApp 运行等。\n而同一条主链的不同侧链也可以借助主链来进行一些交互，这就是借助测链进行跨链的基本原理。\n中继链 # 中继链则是上述侧链和公证人机制的一种综合应用，通过设定跨链交互机制（如 Cosmos 的 IBC）来实现异构链之间的信息共享与交互。需要进行跨链的各个平行链连接到一个中继链来辅助交易的验证和交互。\n跨链技术实践 # 开发实战 # 目前在做一个 BaaS 平台的跨链功能，其基础架构如下：\n子链主要是实现各类业务和应用的链，当子链要与其他链进行跨链业务交互时，它需要执行跨链合约，而我们提供了一个跨链网关来对这些跨链合约进行监听。针对异构区块链。如 Hyperledger Fabric、Ethereum，我们将提供不同的适配器来实现跨链 SDK 与跨链网关之间的交互，适配器提供跨链合约信息查询功能。当另一条业务链的 SDK 接收到跨链合约方法时，如果是合约互调用或数据传递，则直接调用对应的合约方法。\n我主要做的是跨链适配器接口这一部分，适配器作为针对不同链的插件嵌入跨链网关中从而适配不同的应用链，能够很好地协助跨链网关实现对交易的监听、同步与执行。\n而在具体实现中，如在 Fabric 网络中，则是通过子链调用跨链业务合约，而跨链业务合约统一调用一个适配器的合约，在这个适配器合约中，我们实现了交易信息传入，通过 Fabric 事件机制来进行监听（即在合约中实现 SetEvent 方法，而在适配器中对相应事件进行注册，从而实现对跨链合约的监听。\n关于 Fabric 事件监听相关细节及实现详情见 《Hyperledger Fabric Go SDK 事件分析》。\n功能拓展 # 目前趣链科技的 BitXHub 跨链平台是业界实现得比较完善的开源跨链解决方案，其架构如下：\n主要通过中继链、网关和插件机制对跨链流程中的功能、安全性和灵活性等进行了优化，并且设计了 IBTP 链间通用传输协议配合“网关+中继链”的架构来解决跨链交易中的验证、路由等问题。\n总结 # 以上就是对跨链技术的概念梳理与实战总结，为了对跨链机制的各个环节有更深入的了解，之后也将会对目前正在做的跨链服务和 BitXHub 平台进行更深入的剖析和源码解读。\n参考资料 # 关于跨链技术的分析和思考 跨链的简要研究：从原理到技术 跨链技术平台 BitXHub 区块链跨链技术之哈希时间锁 Hyperledger Fabric Go SDK 事件分析 BitXHub Document 十问 BitXHub:谈谈跨链平台的架构设计 "},{"id":14,"href":"/zh/docs/baas/blockchain_baas_platform/","title":"区块链服务平台 (BaaS) 简介及架构","section":"Docs","content":" 区块链服务平台 (BaaS) 简介及架构 # 前言 # 目前工作中负责一个针对 Hyperledger Fabric 的区块链即服务 (Blockchain as a Service, BaaS) 平台的链码管理部分，对这 BaaS 平台的架构与实现很感兴趣，作为一个能为开发者提供一站式应用创建、管理和维护区块链的平台，其架构是怎么样的呢？\n本文是对 BaaS 平台架构的总结和梳理。\nBaaS 简介 # 区块链是一个复杂的分布式系统，尤其是像 Hyperledger Fabric 这样的企业联盟链平台，其部署和运维都非常复杂，作为应用开发者需要处理许多环境问题（如证书、docker 环境等），带来了许多挑战。\n因此，BaaS 平台应运而生，它是一种帮助用户创建、管理和维护企业级区块链的应用平台，用户能够通过友好的 Web 界面对区块链进行操作。通过 BaaS 平台，用户可以很灵活地搭建区块链网络、管理区块链业务和各个模块的功能、进行智能合约的研发和部署以及实时监控和运维。\n通过 BaaS 平台，开发者可以快速进行区块链业务的研发，综合成本大大降低，且有助于系统稳定性、安全性和易用性等的提升。\n平台架构 # BaaS 平台作为一个一站式应用服务，自下而上主要分为以下几层：\n资源层 监控运维层 区块链底层 区块链服务层 应用层 而根据每个系统的业务差异，各个层的架构与功能模块会有所差异，下面将会对几大主流平台层次结构做一些描述。\nHyperledger Cello # Hyperledger Cello 作为 IBM Hyperledger 的顶级项目之一，是一个开源区块链管理平台，支持部署、运行时管理和数据分析等功能。\nCello 目前支持 Hyperledger Fabric 区块链，可以有效管理 Fabric 链的生命周期，主要包含以下模块：\n除了高效地创建部署网络外，Cello 提供了一些对于区块链的管理功能：\n区块链生命周期管理 底层支持多种架构，如 Docker、Swarm、Kubernetes 等 支持多种底层区块链平台并可以自定义配置 支持运行时监控与运维 可插拔的框架设计，可以通过插件的形式拓展第三方功能，如资源调度、驱动代理等 趣链 BaaS # 根据官网介绍，BlocFace 是由趣链科技为企业及开发者全新推出的区块链服务平台，为用户提供一键部署联盟链、可视化监控运维和智能合约研发等一站式研发服务，其平台架构如下：\n总结 # 以上就是对区块链服务平台 (BaaS) 的简介及架构分析，因为目前的 Leader 是 Hyperledger Cello 的项目发起人和核心开发者，鼓励我积极参与 Cello 的开源建设，要加油啦！\n参考资料 # 区块链原理、设计与应用 Hyperledger Cello 项目地址 BlocFace 官网 "},{"id":15,"href":"/zh/docs/blockchain/blockchain_basic/","title":"区块链基础知识与关键技术","section":"Docs","content":" 区块链基础知识与关键技术 # 前言 # 最近对在上 HKU 的\u0026lt;COMP7408 Distributed Ledger and Blockchain Technology\u0026gt;课程，对区块链的基础概念有了更系统的认知，结合之前上过的北京大学肖臻老师《区块链技术与应用》公开课，深知区块链知识体系之庞大，打算更新系列文章对区块链、比特币、以太坊等进行系统的知识梳理，如有错漏，欢迎交流指正。\n区块链中的密码学原理 # 区块链和密码学紧密相关，如比特币采用的核心的公私钥加密技术、数字签名、哈希等，包括很多共识算法也是基于复杂的密码学概念，因此，在开始学习区块链之前，要先了解几个核心的密码学概念，从而能够更深入理解其在区块链体系中的应用。\n哈希函数 # 哈希函数是把一个任意长度的源数据经过一系列算法变成一个固定长度输出值的方法，概念很简单，但其具备的几个特性使它被各个领域广泛应用。\n可以访问这个 Demo 体验一下哈希函数的工作原理（以SHA256为例）！\n第一个特性是单向不可逆性。将一个输入 x 进行哈希运算得到值 H(x)，这一过程很容易，但是如果给定一个值 H(x)，几乎不可能逆推得到 x 的取值，这一特性很好地保护了源数据。\n第二个特性是抗碰撞性。给定一个值 x 和另一个值 y，如果 x 不等于 y，那 H(x) 几乎不可能等于 H(y)，并非完全不可能，但是几率非常低，因此，一个数据的 Hash 值几乎是唯一的，这可以很好地用于身份验证等场景。\n第三个特性是哈希计算不可预测。很难根据现有条件推导出哈希值，但是很容易检验是否正确，这一机制主要应用于PoW挖矿机制中。\n加密/解密 # 加密机制主要分为对称加密和非对称加密两类。\n对称加密机制是两方用同一个密钥来进行信息的加密和解密，很方便，效率也很高，但是密钥的分发存在很大的风险，如果通过网络等方式进行分发，很容易会出现密钥泄漏，从而导致信息泄漏。\n非对称加密机制主要指的是公私钥加密机制，每个人通过算法生成一对密钥，称为公钥和私钥，如果 A 想发送一个信息给 B，可以用 B 的公钥对文件进行加密，将加密后的信息发给 B，这个过程中，即使信息被截获或出现泄漏，也不会暴露源文件，所以可以用任何方式进行传播，当 B 收到加密文件后，用自己的私钥进行解密，从而获取文件内容。B 的私钥没有经过任何渠道进行传播，仅自己知道，所以具备极高的安全性。\n在现实应用中，对很大的文件进行非对称加密效率较低，所以一般采用一种组合机制：假设 A 想发送一个大文件 D 给 B，则先将文件 D 用一个密钥 K 进行对称加密，再用 B 的公钥对密钥 K 进行非对称加密。A 将加密后的密钥 K 和文件 D 发送给 B，期间即使被截获或泄漏，因为没有 B 的私钥，所以无法得到密钥 K，也就无法访问文件 D。B 收到加密后的文件和密钥后，则先用自己的私钥解密得到密钥 K，再用密钥 K 对文件 D 进行解密，从而获取文件内容。\n数字签名 # 数字签名是非对称加密机制的另一种用法，上文讲到每个人拥有一对生成的公钥和私钥，在加密/解密应用中，是用公钥进行加密，用私钥进行解密，而数字签名机制刚好相反，假设一个文件持有者用自己的私钥对文件进行加密，其他人可以用他的公钥进行解密，如果得到结果则可以证明文件的归属权。\n数字签名机制最典型的应用就是比特币区块链网络中，用私钥证明自己对比特币的归属权，对交易进行签名，其他人则可以用公钥来验证交易是否合法，整个过程无需暴露自己的私钥，保障了资产的安全。\n区块链基本概念 # 随着历史的发展，人们的记账方式从单式记账，发展到复式记账、数字记账，最后到分布式记账，因为传统的中心化数字记账则往往依赖于某个或某些组织的可信度，存在一些信任风险，而区块链技术本质上就是一种分布式账本技术，一群人共同维护着一个去中心化的数据库，通过共识机制来共同记账。区块链很容易追溯历史记录，而因为去中心化信任机制的存在，也几乎不可篡改（或者是篡改的成本远远大于收益）。\n相比于传统的数据库，区块链只有增加和查询两种操作，所有的操作历史记录都会准确地保存在账本中且不可变，具备很高的透明度和安全性，当然，代价就是所有节点必须通过一些机制达成共识（因此效率较低，不适合实时性的操作），而且因为每个节点都要永久保存历史记录，会占据很大的存储空间。\n应用场景 # 那怎么判断一个公司/业务是否适合采用区块链作为解决方案呢？\n是否需要数据库？ 是否需要共享写入 是否需要多方达成信任？ 是否能够脱离第三方机构运作？ 是否能够脱离权限机制运作？ 区块链作为一个分布式数据库，主要做的还是信息存储的工作，只是通过其各类机制，在不需要第三方机构介入的前提下让有共同需求但并不互相信任的实体之间也能以相对较低的代价达成一致，从而满足需求，除此之外，系统还有加密认证、高透明度等特性，能够满足一些业务需求。而如果所涉及到的数据不能公开/数据量非常大/需要外部服务来存储数据，或者是业务规则经常发生变化，那区块链就并不适合作为其解决方案。\n因此，在以上的标准下，有如下一些需求很适合区块链作为其解决方案：\n需要建立一个共享的数据库，且有多方参与 参与业务的各方没有建立信任 现有业务信任一个或者多个信任机构 现有业务有加密认证的业务需求 数据需要集成到不同的数据库且业务数字化和一致性的需求迫切 对于系统参与者有统一的规则 多方决策是透明的 需要客观的、不可改变的记录 非实时性处理业务 但其实在很多应用场景里，企业需要在去中心化和效率之间做一些权衡，且有时候很多复杂的业务对透明度、规则都有不同的需求，因此，基于复杂的商业化需求，也有“联盟链”这样的解决方案，能够更好地与现有的系统结合，以满足业务需求。\n区块链类型 # 区块链也有不同的类型，主要有私有链、公有链、联盟链三种。\n私有链主要是应用于某一个特定领域或者只是在某一个企业运行的区块链，主要是用于解决信任问题，如跨部门协作等场景，一般不需要外部机构来访问数据。\n公有链则是公开的交易，往往用于一些需要交易/数据公开的业务，如认证、溯源、金融等场景，比如比特币、以太坊和EOS等。\n联盟链最大的特征是节点需要验证权限才能参与到区块链网络中，而认证一般都是与其现实角色所关联的，因此，联盟链也具有中心化的属性，但效率、拓展性和交易隐私则大大提升了，满足了企业级应用的需求，其中最广泛使用的就是Hyperledger Fabric了。值得一提的是，联盟链往往不需要代币来作为激励，而是将参与的各个节点作为记账节点，通过区块链机制实现跨部门之间的业务协同所带来的经济效益作为内部激励，是一种更健康、更符合企业应用的方式。\n长期来看的话，公有链和联盟链在技术上也会逐渐趋于融合，即使是同一个业务，可以将需要信任的数据放在共有链上，而一些行业数据和私有的数据则可以放在联盟链上，通过权限管理来保障交易隐私。\n区块链基本框架 # 那一个区块链究竟由哪些部分组成呢？\n区块 区块链 P2P 网络 共识机制 \u0026hellip; 区块 # 区块链就是由一个个区块组成的生态系统，每一个区块中包含了前一个区块链的哈希值、时间戳、Merkle Root、Nonce以及区块数据几个部分，比特币的区块大小为 1 MB。可以访问这个 Demo 来体验一下一个区块的生成过程。\n因为每个区块都包含前一个区块的哈希值，根据前文所述的哈希性质，哪怕是极其微小的改变哈希值也会截然不同，因此很容易检测某个区块是否被篡改；Nonce 值则主要是用于调整挖矿难度，可以把时间控制在 10 分钟左右，以保障安全性。\n区块链 # 所有的区块串联起来就形成了区块链，是一个存储着网络中所有交易历史记录的账本，因为每一个区块都包含着上一个区块的哈希信息（比如比特币系统是将上一个区块的块头取两次哈希），因此如果有交易发生变化则会造成区块链断裂，有一个小 Demo 很好地演示了这一过程，大家可以体验一下！\nP2P 网络 # P2P 网络是用于不同用户之间共享信息和资源的一种分布式网络，是一种分布式网络，网络中的每个人都能够得到一份信息备份，而且都有访问权限；而中心化网络是所有人都连接至一个（或一组）中心化网络；去中心化网络是有多个这样的中心网络，但没有一个单点网络可以拥有所有的信息。下图很好地解释了它们之间的区别：\n共识机制 # 区块链网络是由多个网络节点组成的，其中每个节点都存有一份信息备份，那它们是如何对交易达成一致的呢？也就是说，它们作为独立的节点，需要有一种机制来保障互相信任，这就是共识机制。\n常用的共识机制有PoW(Proof of Work)工作量证明，PoS(Proof of Stake)权益证明，DPoS(Delegated Proof of Stake委任权益证明，DBFT(Delegated Byzantine Fault Tolerance)等。\n比特币/以太坊主要采用的是工作量证明机制，通过算力比拼来增加恶意节点的作恶成本。通过动态调整挖矿的难度来让一笔交易时间控制在 10 分钟左右（6 个确认），但随着比特币挖矿越来越火热，消耗资源越来越多，对环境造成破坏；有些矿池拥有大量资源，也会造成一些中心化的风险。\n权益证明机制则是通过权益（一般是代币）持有者进行投票来达成共识。这种机制不需要像工作量证明一样进行大量的算力比拼，但是也有一些风险，称为Nothing at Stake问题，很多权益持有者会在所有区块都投注并从中获利。为了解决这个问题，系统设置了一些规则，如对同时在多个链创建区块的用户/在错误链上创建区块的用户设置一些惩罚机制。目前以太坊正在向这种共识机制转变。\nEOS则采用了委任权益证明，选出一些代表性的节点来进行投票，这种方式目的是优化社区投票的效率和结果，但带来了一些中心化的风险。\nDBFT共识机制则是通过对节点分配不同的角色来达成共识，这样可以很大程度降低开销和避免分叉，但是也有核心角色作恶的风险。\n区块链安全与隐私 # 安全 # 区块链作为一个较新的技术，也存在很多安全隐患，如对数字货币交易所的攻击、智能合约漏洞、对共识协议的攻击、对网络流量（互联网 ISP）的攻击以及上传恶意数据等。比较著名的案例有 Mt.Gox 事件、以太坊 DAO 事件等，因此，对区块链的安全风险也是区块链的重要研究方向。\n可以从协议、加密方案、应用、程序开发和系统等角度进行风险分析，提高区块链应用的安全性。例如在以太坊区块链中，可以对Solidity编程语言、EVM和区块链本身进行一些分析。\n如智能合约中的一种叫低成本攻击的方式，就是通过识别以太坊网络中较低Gas费用的操作，重复执行以破坏整个网络。\n对于安全问题，构建一个通用的代码检测器来检查恶意代码将会是一个更通用的解决方案。\n隐私 # 在讲区块链概念的时候，提到了它很重要的一个特征，隐私性。也就是说，所有人都能看到链上的交易细节和历史记录，这一特性主要应用在食品、药物等供应链环节，但是对于一些金融场景，如个人账户余额、交易信息，则容易造成一些隐私风险。\n那有哪些技术能够应用于这些存在高价值、敏感信息的隐私保护呢？\n硬件层面，可以采用可信的执行环境，采用一些安全硬件，如Intel SGX，很大程度保障了隐私；网络可以采用多路径转发以避免从节点的 ip 地址推算出真实身份。\n在技术层面，混币技术可以把很多交易进行一些混合，这样不容易找出对应的交易发送方和接收方；盲签技术可以保障第三方机构不能将参与交易的双方联系起来；环签用于保障交易签名的匿名性；零知识证明则可以应用于一方（证明者）向另一方（验证者）证明一个陈述是正确的，而无需透露除该陈述是正确的以外的人和信息；同态加密可以保护原数据，给定 E(x)和 E(y)，可以很容易计算出某些关于 x, y 的加密函数值（同态运算）；基于属性的加密（Attribute-based Encryption, ABE）则为各个节点添加一些属性/角色，实现权限控制，从而保护隐私。\n值得注意的是，即使一笔交易生成多个 inputs 和 outputs，这些 inputs 和 outputs 的地址也可能被人关联；除此之外，地址账户和现实世界中的真实身份也可能产生关联。\n总结 # 以上就是对区块链基础知识的一些梳理，主要从概念和原理层面进行了一些学习，后续还会更新对比特币、以太坊、Hyperledger Fabric等典型应用的分析与思考，并对 IPFS、跨链、NFT 等热门技术进行一些探究，敬请期待！\n参考资料 # COMP7408 Distributed Ledger and Blockchain Technology, Professor S.M. Yiu, HKU Udacity Blockchain Developer Nanodegree, Udacity 区块链技术与应用，肖臻，北京大学 区块链技术进阶与实战，蔡亮 李启雷 梁秀波，浙江大学 | 趣链科技 "},{"id":16,"href":"/zh/docs/study_path/","title":"学习路径","section":"Docs","content":" 学习路径 # 区块链涉及很多方面的知识，这是我所推荐的学习路径：\n区块链基础 比特币核心技术 以太坊核心技术 Hyperledger Fabric 等联盟链学习了解 Solidity 智能合约开发 Web3 技术学习与实践 IPFS、跨链等热门技术学习 核心项目源码解读 参与开源项目/项目实战 "}]